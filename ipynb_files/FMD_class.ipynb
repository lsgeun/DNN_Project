{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-21T12:38:43.946800Z","start_time":"2022-07-21T12:38:43.779821Z"},"code_folding":[],"hide_input":false,"id":"yQ6e3v0oCDh1"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","import os\n","import pickle\n","import copy\n","\n","import time\n","from datetime import timedelta\n","\n","'''\n","각 레이어에서 '훈련 피처 맵'과 '정분류 테스트 피처 맵'과 '오분류 테스트 피처 맵'은 모두 같은 형태이어야 한다.\n","예를 들어, 첫번째 레이어에서 훈련 피처 맵(100, 12, 12) 정분류 테스트 피처 맵(30, 12, 12), 오분류 테스트 피처 맵(20, 12, 12)\n","와 같이 인덱스 첫번째인 개수는 다르더라도 이차 넘파이 배열은 같아야 한다.\n","'''\n","\n","class FMD():\n","    # directory\n","    # root_dir: 관련된 데이터가 모두 저장되어 있는 디렉토리 경로\n","    # origin_dir: train, rtest, wtest가 있는 디렉토리\n","    # eval_dir: 거리를 잴 데이터가 있는 디렉토리\n","    root_dir=\"\"; origin_dir=\"\"; eval_dir=\"\"\n","\n","    # data_infos\n","    # 데이터 타입에 대한 이름\n","    origin_names = [\"train\", \"rtest\", \"wtest\"]\n","    eval_names = []\n","    # (이하) 대괄호로 쳐져있는 배열은 나중에 넘파이 배열이 될 수 있다.\n","    # (origin_, eval_)K: 피처 맵의 개수, L: 레이어의 개수(0~L-1는 각 레이어의 인덱스)\n","    # shape: 레이어의 넘파이 모양\n","    # 아래 (origin_, eval_)K, L, shape을 'data_infos'라고 하자.\n","    # min max은 정규화된 데이터의 최소값과 최대값을 의미한다.\n","    origin_K={\"train\": 0, \"rtest\": 0, \"wtest\": 0}; eval_K={}; L=0; shape=[]\n","    \n","    # FM_means\n","    # (이하) 훈련 데이터, 정분류 테스트 데이터, 오분류 테스트 데이터에서 데이터라는 용어를 생략하고 적었다.\n","    # 아래의 배열들은 모두 파이썬 배열이고 첫 번째 인자에 layer가 들어간다.\n","    # TFM_mean: 훈련 평균 피처 맵(베이스 피처 맵)\n","    # RFM_mean: 정분류 테스트 평균 피처 맵\n","    # WFM_mean: 오분류 테스트 평균 피처 맵\n","    TFM_mean=[]; RFM_mean=[]; WFM_mean=[]\n","\n","    # AMs와 그와 관련된 것\n","    # TAM: 훈련 활성화 피처 맵       , RAM: 정분류 테스트 활성화 피처 맵\n","    # WAM: 오분류 테스트 활성화 피처 맵\n","    TAM=[]; RAM=[]; WAM=[]\n","    # alpha는 거리 계산을 위한 인덱스를 고르기 위해 필요한 변수이다.\n","    # w_minus_r_max(= max(w-r))는 나중에 alpha 값에 의존하여 최대로 만들어진다.\n","    # alpha_min, alpha_max는 각 레이어에서의 alpha가 가질 수 있는 최소값 최대값을 나타낸 것이다.\n","    # [hyper parameter] alpha_slice은 alpha_min에서 alpha_max로 몇 번의의 간격으로 도착할지 알려주는 변수임.\n","    alpha=[]; alpha_slice=[]; w_minus_r_max=[]\n","    alpha_min=[]; alpha_max=[]\n","    # DAM_indexes는 나중에 거리 계산할 때 쓰이는 다차원 인덱스들의 집합이다.\n","    # 각 원소는 피처 맵의 한 원소의 인덱스를 나타낸다.\n","    # DAM: 거리 활성화 맵, DAM는 거리 계산을 위한 인덱스만 활성된 맵이다.\n","    # 각 레이어마다 튜플들 세트가 있어야 함. np.array의 item 메소드를 사용할 것이기 때문\n","    # [hyper parameter] DAM_select는 DAM를 고르는 방법을 알려줌.\n","    DAM_indexes=[]; DAM=[]; DAM_select=[]\n","\n","    # index_infos\n","    '''\n","    사용하지 않음.\n","    '''\n","    # '오분류 테스트' 각 인덱스에서의 거리에 대한 중간값, 평균, 표준편차, 최소값, 최대값에 대한 배열은\n","    # 각 mid, mean, std, min, max임\n","    # 인덱스마다 달라지는 값이니 총칭해서 'index_infos'라고 하자.\n","    mid=[]; mean=[]; std=[]; min=[]; max=[]\n","    \n","    # layer_infos\n","    '''\n","    사용하지 않음.\n","    lfmd_select: 레이어 피처맵 거리 계산 방법\n","    [hyper parameter] lfmd_select는 각 레이어에 대한 피처 맵을 구하는 방법을 저장한다.\n","    '''\n","    lfmd_select=[]\n","    # W: 레이어 피처맵 거리 가중치\n","    # [hyper parameter] W는 각 레이어의 피처 맵에 곱할 weight 중요도이다\n","    # 레이어 피처 맵에 대한 가시적인 출력을 위해 1차 이상의 배열을 2차원 배열로 바꿈\n","    # square_FM_side은 1차 이상의 배열을 2차원 배열로 바꾸기 위해 필요한 2차원 배열의 한 변의 길이임.\n","    square_FM_side=[]; W=[]\n","\n","    # fmdc\n","    # fmdc 피처 맵 거리 기준으로 어떤 데이터가 나중에 오분류 될 거 같은지 판단함.\n","    # fmdc는 거리이므로 음수가 될 수 없음, 따라서, fmdc가 -1이라는 것은 아직 계산하지 않았다는 의미임.\n","    fmdc=-1\n","\n","    def __init__(self, root_dir_=\"\"):\n","        # root 디렉토리 입력이 없다면 return\n","        if root_dir_==\"\":\n","            print(\"루트 디렉토리 경로를 설정해주세요.\")\n","            return\n","        # root 디렉토리\n","        self.root_dir = root_dir_\n","\n","        # origin 디렉토리\n","        self.origin_dir = f\"{self.root_dir}/origin\"\n","        # 훈련을 저장하는 디렉토리\n","        self.train_dir=f\"{self.origin_dir}/{self.origin_names[0]}\"\n","        # 정분류 테스트를 저장하는 디렉토리\n","        self.rtest_dir=f\"{self.origin_dir}/{self.origin_names[1]}\"\n","        # 오분류 테스트를 저장하는 디렉토리\n","        self.wtest_dir=f\"{self.origin_dir}/{self.origin_names[2]}\"\n","        \n","        # eval 디렉토리\n","        self.eval_dir = f\"{self.root_dir}/eval\"\n","\n","        # os.paht.isdir는 '\\ ' 대신, ' '을 써도 됨\n","        is_there_instances = os.path.isdir(f\"{self.root_dir}/instances\")\n","        # 객체를 불러오거나 저장할 디렉토리 생성\n","        # ' '을 '\\ '로 바꿈\n","        root_dir = self.root_dir.replace(' ', '\\ ')\n","        # instances 디렉토리가 없을 경우만 instances 생성\n","        if not is_there_instances:\n","            os.system(f\"mkdir {root_dir}/instances\")\n","\n","    def fit(self):\n","        # 객체의 속성 초기화\n","        # root_dir의 데이터가 올바르다면 init이 에러 없이 완료됨\n","        self.set_data_infos()\n","        self.set_FM_means()\n","        self.set_AMs()\n","        # self.set_index_infos()\n","        self.set_fmdc()\n","\n","    def set_root_dir(self, root_dir_):\n","        self.root_dir = root_dir_\n","\n","        # origin 디렉토리\n","        self.origin_dir = f\"{self.root_dir}/origin\"\n","        # 훈련을 저장하는 디렉토리\n","        self.train_dir=f\"{self.origin_dir}/{self.origin_names[0]}\"\n","        # 정분류 테스트를 저장하는 디렉토리\n","        self.rtest_dir=f\"{self.origin_dir}/{self.origin_names[1]}\"\n","        # 오분류 테스트를 저장하는 디렉토리\n","        self.wtest_dir=f\"{self.origin_dir}/{self.origin_names[2]}\"\n","        \n","        # eval 디렉토리\n","        self.eval_dir = f\"{self.root_dir}/eval\"\n","\n","    def create_practice(self):\n","        '''\n","        아마도 나중에 adapter를 만들 때 사용될 수 있을 것 같음.\n","        '''\n","\n","        # 연습 데이터 랜덤 시드 설정\n","        random_seed_id = 42\n","        np.random.seed(random_seed_id)\n","        # 연습 데이터 범위 설정\n","        '''\n","        연습 데이터 범위가 0~1로로 고정, np.random.rand(or random)으로 바로 구현 가능\n","        '''\n","        # random_start = 1; random_end = 2\n","\n","        # 디렉토리 설정\n","        root_dir = \"practice\"\n","        origin_dir = f\"{root_dir}/origin\"\n","        eval_dir = f\"{root_dir}/eval\"\n","        # 디렉토리 생성\n","        os.system(f\"mkdir {root_dir}\")\n","        os.system(f\"mkdir {origin_dir}\")\n","        os.system(f\"mkdir {eval_dir}\")\n","\n","        # origin 데이터 종류 적기\n","        origin_names=\"train rtest wtest\"\n","        # origin_K 각 데이터 종류마다의 개수\n","        origin_K=\"10000 2000 4000\"\n","        # eval 데이터 종류 적기\n","        eval_names=\"rotation_90 brightness_10 whiteness_10\"\n","        # eval_K 각 데이터 종류마다의 개수\n","        eval_K=\"12000 4000 6000\"\n","        # L: 레이어의 개수, shape: 레이어의 모양\n","        L=\"3\";\n","        shape = \"12 12\" + \"\\n\"\n","        shape += \"12 2 3 4\" + \"\\n\"\n","        shape += \"24 8\"\n","\n","        # data_infos.txt 파일로 저장\n","        # origin 데이터 종류는 train rtest wtest로 이미 정해져 있으므로 파일에 저장하지 않음\n","        os.system(f\"touch {root_dir}/data_infos.txt\")\n","        data_infos_txt = open(f\"{root_dir}/data_infos.txt\", 'w')\n","        data_infos_txt.write(origin_K + \"\\n\")\n","        data_infos_txt.write(eval_names + \"\\n\")\n","        data_infos_txt.write(eval_K + \"\\n\")\n","        data_infos_txt.write(L + \"\\n\")\n","        data_infos_txt.write(shape)\n","        data_infos_txt.close()\n","\n","        # 자료구조를 바꾸어 for문 활용을 쉽게 하기\n","        origin_names = origin_names.split()\n","        origin_K = list(map(int,origin_K.split()))\n","        origin_K_list = origin_K\n","        origin_K = {}\n","        for i in range(len(origin_names)):\n","            origin_K[origin_names[i]] = origin_K_list[i]\n","        \n","        eval_names = eval_names.split()\n","        eval_K = list(map(int,eval_K.split()))\n","        eval_K_list = eval_K\n","        eval_K = {}\n","        for i in range(len(eval_names)):\n","            eval_K[eval_names[i]] = eval_K_list[i]\n","\n","        L=int(L)\n","        \n","        shape = shape.split('\\n')\n","        for ith in range(len(shape)):\n","            shape[ith] = list(map(int,shape[ith].split()))\n","\n","        # origin 연습 데이터 생성\n","        for origin_name in origin_names:\n","            os.system(f\"mkdir {origin_dir}/{origin_name}\")\n","            # datas: 데이터들을 최대 1000개씩 담음.\n","            # file_index: 0 ~ K-1 까지 파일을 1000개씩 담을건데 0에서부터 시작해서 1000개마다 1 증가함\n","            datas = []; file_index = 0\n","\n","            for k in range(origin_K[origin_name]):\n","                # data: 데이터는 레이어들을 담고 있음\n","                data = []\n","                # data에 레이어들을 담은 후\n","                for l in range(L):\n","                    lth_layer = np.random.random(shape[l])\n","                    data.append(lth_layer)\n","                # datas에 레이어들을 담은 data를 담음\n","                datas.append(data)\n","\n","                # 1000개마다 데이터가 저장됨\n","                if len(datas) == 1000:\n","                    # file_index번째 datas를 저장\n","                    with open(f'{origin_dir}/{origin_name}/{origin_name}_{file_index}.pickle', 'wb') as f:\n","                        pickle.dump(datas, f, pickle.HIGHEST_PROTOCOL)\n","                    # datas의 데이터를 램에서 제거\n","                    del datas\n","                    # datas를 빈 배열로 만듦\n","                    datas = []\n","                    # 파일 인덱스 1 증가\n","                    file_index += 1\n","\n","            # datas의 데이터가 남아있다면, 1~999개의 데이터가 남아있다면 그 데이터도 마저 저장함\n","            if len(datas) != 0:\n","                with open(f'{origin_dir}/{origin_name}/{origin_name}_{file_index}.pickle', 'wb') as f:\n","                    pickle.dump(datas, f, pickle.HIGHEST_PROTOCOL)\n","\n","            # 훈련 데이터의 경우\n","            # 정분류(1), 오분류(0) 여부를 알려주는 넘파이 배열을 파일로 저장함\n","            if origin_name == \"train\":\n","                origin_result = np.random.randint(0,2, origin_K[origin_name])\n","                # 1 -> True, 0 -> False로 변경\n","                origin_result = np.array(origin_result, dtype=\"bool\")\n","                np.save(f\"{origin_dir}/{origin_name}/{origin_name}_result.npy\", origin_result)\n","\n","        # eval 연습 데이터 생성\n","        for eval_name in eval_names:\n","            os.system(f\"mkdir {eval_dir}/{eval_name}\")\n","            # datas: 데이터들을 최대 1000개씩 담음.\n","            # file_index: 0 ~ K-1 까지 파일을 1000개씩 담을건데 0에서부터 시작해서 1000개마다 1 증가함\n","            datas = []; file_index = 0\n","\n","            for k in range(eval_K[eval_name]):\n","                # data: 데이터는 레이어들을 담고 있음\n","                data = []\n","                # data에 레이어들을 담은 후\n","                for l in range(L):\n","                    lth_layer = np.random.random(shape[l])\n","                    data.append(lth_layer)\n","                # datas에 레이어들을 담은 data를 담음\n","                datas.append(data)\n","\n","                # 1000개마다 데이터가 저장됨\n","                if len(datas) == 1000:\n","                    # file_index번째 datas를 저장\n","                    with open(f'{eval_dir}/{eval_name}/{eval_name}_{file_index}.pickle', 'wb') as f:\n","                        pickle.dump(datas, f, pickle.HIGHEST_PROTOCOL)\n","                    # datas의 데이터를 램에서 제거\n","                    del datas\n","                    # datas를 빈 배열로 만듦\n","                    datas = []\n","                    # 파일 인덱스 1 증가\n","                    file_index += 1\n","\n","            # datas의 데이터가 남아있다면, 1~999개의 데이터가 남아있다면 그 데이터도 마저 저장함\n","            if len(datas) != 0:\n","                with open(f'{eval_dir}/{eval_name}/{eval_name}_{file_index}.pickle', 'wb') as f:\n","                    pickle.dump(datas, f, pickle.HIGHEST_PROTOCOL)\n","\n","            # 정분류(1), 오분류(0) 여부를 알려주는 넘파이 배열을 파일로 저장함\n","            eval_result = np.random.randint(0,2, eval_K[eval_name])\n","            # 1 -> True, 0 -> False로 변경\n","            eval_result = np.array(eval_result, dtype=\"bool\")\n","            np.save(f\"{eval_dir}/{eval_name}/{eval_name}_result.npy\", eval_result)\n","\n","    def set_data_infos(self):\n","        # root dir의 data_infos.txt 열기\n","        data_infos = open(f\"{self.root_dir}/data_infos.txt\", 'r')\n","        data_infos_strs = data_infos.read()\n","        data_infos_str_list = data_infos_strs.split('\\n')\n","        # 0th: origin_K \n","        origin_K = list(map(int, data_infos_str_list[0].split()))\n","        origin_name_K_zip = zip(self.origin_names, origin_K) \n","        for origin_name, origin_K in origin_name_K_zip:\n","            self.origin_K[origin_name] = origin_K\n","        # 1th: eval_names\n","        self.eval_names = data_infos_str_list[1].split()\n","        # 2th: eval_K\n","        eval_K = list(map(int, data_infos_str_list[2].split()))\n","        eval_name_K_zip = zip(self.eval_names, eval_K) \n","        for eval_name, eval_K in eval_name_K_zip:\n","            self.eval_K[eval_name] = eval_K\n","        # 3th: L\n","        self.L = int(data_infos_str_list[3])\n","        # 4+0th ~ 4+(L-1)th: shape\n","        for l in range(self.L):\n","            shape_l = list(map(int,data_infos_str_list[4+l].split()))\n","            self.shape.append(shape_l)\n","        # root dir의 data_infos.txt 닫기\n","        data_infos.close()\n","\n","        # 레이어 피처 맵을 구하는 방식을 se_lfmd 모두 통일\n","        '''\n","        for l in range(self.L):\n","            self.lfmd_select.append(\"se_lfmd\")\n","        '''\n","        # W를 균등하게 만듦\n","        for l in range(self.L):\n","            self.W.append(1/self.L)\n","\n","    def set_FM_means(self):\n","        # 인스턴스 속성을 변수로 포인터처럼 가르킴\n","        train = self.origin_names[0]; rtest = self.origin_names[1]; wtest = self.origin_names[2]\n","        L = self.L; shape = self.shape\n","\n","        def set_FM_mean(origin):\n","            # 인스턴스 속성을 변수로 포인터처럼 가르킴\n","            origin_K = self.origin_K[origin]\n","\n","            if origin == train:\n","                OFM_mean = self.TFM_mean; origin_dir = self.train_dir;\n","            elif origin == rtest:\n","                OFM_mean = self.RFM_mean; origin_dir = self.rtest_dir;\n","            elif origin == wtest:\n","                OFM_mean = self.WFM_mean; origin_dir = self.wtest_dir;\n","            else:\n","                print('잘못된 origin: ', origin, sep='')\n","                return\n","\n","            # 각 레이어의 피처 맵을 0으로 초기화하여 생성\n","            for l in range(L):\n","                OFM_mean_l = np.zeros(shape[l])\n","                OFM_mean.append(OFM_mean_l)\n","\n","            # OFMP_k는 k번째 origin 데이터가 속한 FMP임\n","            # k번 째 데이터의 OFMPI는 OFMPI_k = k // 1000임\n","            # k번 째 데이터의 OFMPO은 OFMPO_k = k % 1000임\n","            OFMP_k = None; prev_OFMPI_k = None; cur_OFMPI_k = None\n","\n","            k = 0\n","            # 0번 째 데이터가 속한 OFMP_k를 불러들인 후\n","            prev_OFMPI_k = k // 1000\n","            OFMPO_k = k % 1000\n","            with open(f'{origin_dir}/{origin}_{prev_OFMPI_k}.pickle', 'rb') as f:\n","                OFMP_k = pickle.load(f)\n","            # 0번 째 데이터를 OFM_mean에 넣는다.\n","            for l in range(L):\n","                OFM_k_l = OFMP_k[OFMPO_k][l]\n","                OFM_mean[l] = OFM_mean[l] + OFM_k_l\n","\n","            # k = 1 ~ K-1\n","            # 1~K-1번 째 데이터로 OFM_mean을 구한다.\n","            for k in range(1, origin_K):\n","                # k번 째 데이터의 OFMPI, OFMPO 구함\n","                cur_OFMPI_k = k // 1000\n","                OFMPO_k = k % 1000\n","\n","                # OFMP_k가 이미 있다면 가지고 오지 않고\n","                #             없다면 이전 OFMP를 지우고 현재 OFMP를 가지고 온다.\n","\n","                # cur_OFMPI_k와 prev_OFMPI_k가 같다면\n","                if cur_OFMPI_k == prev_OFMPI_k:\n","                    pass # 아무 작업 하지 않고\n","\n","                # cur_OFMPI_k와 prev_OFMPI_k가 다를 경우\n","                else:\n","                    # 이전 OFMP_k의 기억공간을 램에서 제거한 후\n","                    del OFMP_k\n","                    # cur_OFMPI_k를 현재 OFMP_k를 가지고 온다.\n","                    with open(f'{origin_dir}/{origin}_{cur_OFMPI_k}.pickle', 'rb') as f:\n","                        OFMP_k = pickle.load(f)\n","\n","                # prev_OFMPI_k를 cur_OFMPI_k로 초기화 \n","                prev_OFMPI_k = cur_OFMPI_k\n","\n","                for l in range(L):\n","                    OFM_k_l = OFMP_k[OFMPO_k][l]\n","                    OFM_mean[l] = (OFM_mean[l]*k + OFM_k_l)/(k+1)\n","\n","        # 훈련, 정분류 테스트, 오분류 테스트 데이터에 대한 FM_mean을 구함\n","        set_FM_mean(train); set_FM_mean(rtest); set_FM_mean(wtest)\n","        \n","    def set_AMs(self):\n","        # alpha, w_minus_r_max를 0으로 초기화\n","        self.alpha = np.zeros(self.L)\n","        self.w_minus_r_max = np.zeros(self.L)\n","\n","        # [hyperparameter] alpha_slice를 일단 1000으로 함\n","        for l in range(self.L):\n","            self.alpha_slice.append(1000)\n","\n","        # alpha의 최소값과 최대값을 구함\n","        self.alpha_min = np.zeros(self.L)\n","        self.alpha_max = np.zeros(self.L)\n","        for l in range(self.L):\n","            self.alpha_min[l] = np.array([self.RFM_mean[l].min(),\n","                                         self.TFM_mean[l].min(),\n","                                         self.WFM_mean[l].min()]).min()\n","            self.alpha_max[l] = np.array([self.RFM_mean[l].max(),\n","                                         self.TFM_mean[l].max(),\n","                                         self.WFM_mean[l].max()]).max()\n","\n","        # TAM, RAM, WAM을 0으로 초기화 함\n","        for l in range(self.L):\n","            TAM_l = np.zeros(self.shape[l])\n","            self.TAM.append(TAM_l)\n","        for l in range(self.L):\n","            RAM_l = np.zeros(self.shape[l])\n","            self.RAM.append(RAM_l)\n","        for l in range(self.L):\n","            WAM_l = np.zeros(self.shape[l])\n","            self.WAM.append(WAM_l)\n","\n","        # w-r가 최대가 되는 alpha, TAM, RAM, WAM을 찾음\n","        for l in range(self.L):\n","            # range 부분 고칠 필요가 있음\n","            a_min_l = self.alpha_min[l]; a_max_l = self.alpha_max[l]\n","            a_slice_l = self.alpha_slice[l]\n","            interval_l = (a_max_l - a_min_l)/a_slice_l\n","            # range(a_slice_l+1) 해야 a_min_l 부터 a_max_l 까지 감\n","            for alpha_l in [a_min_l + interval_l*s for s in range(a_slice_l+1)]:\n","                TAM_l = np.array(self.TFM_mean[l] > alpha_l)\n","                RAM_l = np.array(self.RFM_mean[l] > alpha_l)\n","                WAM_l = np.array(self.WFM_mean[l] > alpha_l)\n","                \n","                TAM_l_xor_RAM_l = np.logical_xor(TAM_l, RAM_l)\n","                TAM_l_xor_WAM_l = np.logical_xor(TAM_l, WAM_l)\n","                # r_l은 TAM_l과 RAM_l이 얼마나 유사하지 않은지 보여준다.\n","                # 즉, TAM_l과 RAM_l이 유사하지 않을수록 r_l 값이 커진다.\n","                # w_l도 마찬가지이다.\n","                r_l = len(np.where(TAM_l_xor_RAM_l == True)[0])\n","                w_l = len(np.where(TAM_l_xor_WAM_l == True)[0])\n","                \n","                if w_l - r_l > self.w_minus_r_max[l]:\n","                    # w-r가 이전의 w-r보다 클 때\n","                    # 즉, TAM과 RAM이 더 유사해지거나 TAM과 WAM이 더 다를 때\n","                    # alpha, w-r, TAM, RAM, WAM를 최신화함.\n","                    self.w_minus_r_max[l] = w_l - r_l\n","                    self.alpha[l] = alpha_l\n","                    self.TAM[l] = TAM_l\n","                    self.RAM[l] = RAM_l\n","                    self.WAM[l] = WAM_l\n","\n","        # TAM, RAM, WAM를 이용하여 DAM를 구함\n","        # DAM에 WAM를 deep copy 복사함\n","        for l in range(self.L):\n","            DAM_l = self.WAM[l].copy()\n","            self.DAM.append(DAM_l)\n","\n","        # [hyperparameter] 일단 모든 레이어를 'and' 방식으로 함\n","        for l in range(self.L):\n","            self.DAM_select.append(\"and\")\n","\n","        # 선택하려는 방식('and', 'or', 등등)대로 DAM를 고름\n","        for l in range(self.L):\n","            # 'and' 방식보다 'or' 방식에서 대부분의 인덱스에서 오분류 데이터의 값이 상대적으로 더 커짐\n","            # 다만 'or' 방식은 'and' 방식보다 거리 계산을 위한 인덱스의 개수가 더 줄어듦\n","            # 'all' 방식은 모든 인덱스를 다 사용함.\n","            if self.DAM_select[l] == \"and\":\n","                TAM_l_and_RAM_l = np.logical_and(self.TAM[l], self.RAM[l])\n","                # WAM에서 (TAM 교집합 RAM)과 곂치는 부분을 False로 만듦\n","                np.place(self.DAM[l], TAM_l_and_RAM_l, False)\n","            elif self.DAM_select[l] == \"or\":\n","                TAM_l_or_RAM_l = np.logical_or(self.TAM[l], self.RAM[l])\n","                # WAM에서 (TAM 합집합 RAM)과 곂치는 부분을 False로 만듦)\n","                np.place(self.DAM[l], TAM_l_or_RAM_l, False)\n","            elif self.DAM_select[l] == \"all\":\n","                not_DAM_l = np.logical_not(self.DAM[l])\n","                # 모든 인덱스를 다 사용함\n","                np.place(self.DAM[l], not_DAM_l, True)\n","\n","        # DAM_indexes를 지정함\n","        for l in range(self.L):\n","            nonzero_DAM_l = np.nonzero(self.DAM[l])\n","            DAM_indexes_l = np.empty((1,len(nonzero_DAM_l[0])), dtype=\"int32\")\n","            # l 레이어 차원의 수 만큼 각 차원에 대한 인덱스들을 DAM_indexes_l에 삽입\n","            for i in range(len(nonzero_DAM_l)):\n","                DAM_indexes_l = np.append(DAM_indexes_l, nonzero_DAM_l[i].reshape(1,-1), axis=0)\n","            # 처음 배열은 np.empty 메소드로 만들어진 쓰레기 값이라 버림\n","            # 가로 방향이라 세로 방향으로 길게 늘어지도록 바꿈\n","            DAM_indexes_l = list(DAM_indexes_l[1:].T)\n","            # DAM_indexes_l 각 원소가 리스트 형태인데 그것을 튜플로 바꿈\n","            # 튜플로 만드는 이유는 np.item() 메소드가 튜플을 인자로 받기 때문\n","            for i in range(len(DAM_indexes_l)):\n","                DAM_indexes_l[i] = tuple(DAM_indexes_l[i])\n","\n","            self.DAM_indexes.append(DAM_indexes_l)\n","    \n","    # 아래의 방식을 쓴다면 거리 계산하기 전에 걸리는 시간도 길어지고 그다지 효율적이지도 않음.\n","    # 따라서 index_infos를 구하지 않음\n","    def set_index_infos(self):\n","        wtest = self.origin_names[2]\n","\n","        for l in range(self.L):\n","            # mid_l, mean_l, std_l, min_l, max_l를 모두 -1로 초기화\n","            # 거리가 -1(음수)이 될 수 없으므로 -1인 부분은 초기화 되지 않은 인덱스 값임\n","            mid_l = np.zeros(self.shape[l]); mid_l = mid_l - 1\n","            mean_l = np.zeros(self.shape[l]); mean_l = mean_l - 1\n","            std_l = np.zeros(self.shape[l]); std_l = std_l - 1\n","            min_l = np.zeros(self.shape[l]); min_l = min_l - 1\n","            max_l = np.zeros(self.shape[l]); max_l = max_l - 1\n","            for DAM_index_l in self.DAM_indexes[l]:\n","                ED_array_l_DAM_index = np.empty((0), dtype=\"float\")\n","                for k in range(self.origin_K[wtest]):\n","                    WFM_k_l = np.load(f\"{self.wtest_dir}/{wtest}_{k}_{l}.npy\")\n","                    # append를 많이 사용하면 속도가 줄어듦\n","                    ED_array_l_DAM_index = np.append(ED_array_l_DAM_index, abs(self.TFM_mean[l].item(DAM_index_l)\n","                                                                                 - WFM_k_l.item(DAM_index_l))) # TFM_mean = BFM\n","\n","                mid_l.itemset(DAM_index_l, sorted(ED_array_l_DAM_index)[self.origin_K[wtest] // 2])\n","                mean_l.itemset(DAM_index_l, ED_array_l_DAM_index.mean())\n","                std_l.itemset(DAM_index_l, ED_array_l_DAM_index.std())\n","                min_l.itemset(DAM_index_l, ED_array_l_DAM_index.min())\n","                max_l.itemset(DAM_index_l, ED_array_l_DAM_index.max())\n","\n","            self.mid.append(mid_l)\n","            self.mean.append(mean_l)\n","            self.std.append(std_l)\n","            self.min.append(min_l)\n","            self.max.append(max_l)\n","    # 사용하지 않음\n","    def md_lfmd(self, dir1, dir2, k, l):\n","        '''\n","        중앙값으로 나눠서 구한 레이어 거리(middle division layer feature map distance)\n","\n","        dir1은 root 디렉토리 기준으로 깊이가 1인 것\n","        dir2은 root 디렉토리 기준으로 깊이가 2인 것\n","        k는 몇 번째 데이터인지 알려줌, l은 몇번째 레이어인지 알려줌\n","        예시: MDD(\"origin\", \"train\", 1, 2)은\n","        root 디렉토리/origin/train/train_1_2에 대한 레이어 거리임.\n","        '''\n","        md_lfmd = 0\n","        dir2_FM_k_l = np.load(f'{self.root_dir}/{dir1}/{dir2}/{dir2}_{k}_{l}.npy')\n","\n","        for index in self.DAM_indexes[l]:\n","            length_index = abs(self.TFM_mean[l].item(index) - dir2_FM_k_l.item(index))\n","            mid_index = self.mid[l].item(index)\n","\n","            md_lfmd += (length_index / mid_index)**2\n","        \n","        return md_lfmd\n","    # 사용하지 않음\n","    def norm_lfmd(self, dir1, dir2, k, l):\n","        '''\n","        정규화를 시켜서 구한 레이어 거리(normal layer feature map distance)\n","        '''\n","        norm_lfmd = 0\n","        dir2_FM_k_l = np.load(f'{self.root_dir}/{dir1}/{dir2}/{dir2}_{k}_{l}.npy')\n","        \n","        for index in self.DAM_indexes[l]:\n","            length_index = abs(self.TFM_mean[l].item(index) - dir2_FM_k_l.item(index))\n","            mean_index = self.mean[l].item(index)\n","            std_index = self.std[l].item(index)\n","\n","            norm_lfmd += math.exp((length_index - mean_index) / std_index)\n","        \n","        return norm_lfmd\n","    # 사용하지 않음\n","    def mms_lfmd(self, dir1, dir2, k, l):\n","        '''\n","        min-max으로 표준화한 후 shift하여 구한 레이어 거리(min max shift layer feature map distance)\n","        '''\n","        mms_lfmd = 0\n","        dir2_FM_k_l = np.load(f'{self.root_dir}/{dir1}/{dir2}/{dir2}_{k}_{l}.npy')\n","        \n","        for index in self.DAM_indexes[l]:\n","            length_index = abs(self.TFM_mean[l].item(index) - dir2_FM_k_l.item(index))\n","            mid_index = abs(self.TFM_mean[l].item(index) - self.mid[l].item(index))\n","            min_index = self.min[l].item(index)\n","            max_index = self.max[l].item(index)\n","\n","            min_max_index = (length_index - min_index) / (max_index - min_index)\n","            min_max_mid_index = (mid_index - min_index) / (max_index - min_index)\n","            \n","            if length_index <= mid_index:\n","                mms_lfmd += (min_max_index + 0)**2\n","            else:\n","                mms_lfmd += (min_max_index + 1 - min_max_mid_index)**2\n","        \n","        return mms_lfmd\n","    \n","    def se_lfmd(self, FM_k_l, l, percent=50):\n","        '''\n","        se_lfmd: shift exponential layer feature map distance\n","        일단 디폴트로 length_min length_max의 50(정중앙 값)에 해당하는 부분을 origin(원점)으로 이동\n","        '''\n","        se_lfmd = 0\n","        \n","        # self.TFM_mean[l], FM_k_l를 norm_min, norm_max으로 정규화\n","        # norm_min과 norm_max을 1. 속성으로 두거나 2. 형식 매개변수로 둬도 되는데 3. 일단 지역 변수로 두자.\n","        # 이것도 [hyper parameter]가 될 수도 있겠다.\n","        norm_min = -1; norm_max = 1\n","        TFM_mean_l_norm = self.normalize_layer(self.TFM_mean[l], norm_min, norm_max)\n","        FM_k_l_norm = self.normalize_layer(FM_k_l, norm_min, norm_max)\n","\n","        # lengths: 인덱스 마다 TFM_mean_norm과 FM_k_l_norm 사이의 거리(절대값)를 구함\n","        lengths = abs(TFM_mean_l_norm - FM_k_l_norm)\n","        \n","        # 'shift_value'을 구함\n","        # norm_min, norm_max가 같은 두 값의 길이의 min(length_min)은 0이고\n","        # length_max는 norm_max - norm_min임\n","        length_max = norm_max - norm_min; length_min = 0\n","        # \n","        length_interval_max = length_max - length_min\n","        length_interval_percent = length_interval_max * (percent/100)\n","        # value_to_be_origin는 나중에 원점이 될 값임\n","        value_to_be_origin = length_min + length_interval_percent\n","        # shift_value는 value_to_be_origin을 0으로 옮기기 위한 이동 값임\n","        shift_value = -value_to_be_origin\n","\n","        # 각 원소를 shift value 만큼 이동시키고 'exponential'을 취함\n","        for index in self.DAM_indexes[l]:\n","            exp_lengths_minus_shift_value = np.exp(lengths - shift_value)\n","        # se를 취한 값들을 모두 더한 것이 se_lfmd임\n","        se_lfmd = exp_lengths_minus_shift_value.sum()\n","        \n","        return se_lfmd\n","    \n","    def normalize_layer(self, layer, min, max):\n","        '''\n","        'layer의 min, max'으로 layer를 'min-max 정규화'를 한 후\n","        최소값이 min, 최대값이 max가 되도록 layer를 정규화 한다.\n","        layer(넘파이)에 스칼라 곱셈과 스칼라 덧셈을 적용하여 구현할 수 있다.\n","        '''\n","        # 'layer의 min, max'으로 layer를 'min-max 정규화'\n","        layer_min = layer.min(); layer_max = layer.max();\n","        # layer 값들이 하나라도 다르다면\n","        if layer_max - layer_min != 0:\n","            layer = (layer - layer_min) / (layer_max - layer_min)\n","        # layer 값들이 모두 같다면\n","        else:\n","            layer = 0\n","\n","        scalar_multiplyer = max - min\n","        scalar_adder = min\n","\n","        normalized_layer = scalar_multiplyer*layer + scalar_adder\n","\n","        return normalized_layer\n","    \n","    def ab_lfmd(self, FM_k_l, l):\n","        '''\n","        ab_lfmd: absolute layer feature map distance\n","        레이어의 인덱스들 간의 절대값을 모두 더한다.\n","        '''\n","        ab_lfmd = 0\n","        \n","        # self.TFM_mean[l], FM_k_l를 norm_min, norm_max으로 정규화\n","        # norm_min과 norm_max을 1. 속성으로 두거나 2. 형식 매개변수로 둬도 되는데 3. 일단 지역 변수로 두자.\n","        # 이것도 [hyper parameter]가 될 수도 있겠다.\n","        norm_min = -1; norm_max = 1\n","        TFM_mean_l_norm = self.normalize_layer(self.TFM_mean[l], norm_min, norm_max)\n","        FM_k_l_norm = self.normalize_layer(FM_k_l, norm_min, norm_max)\n","\n","        # lengths: 인덱스 마다 TFM_mean_norm과 FM_k_l_norm 사이의 거리(절대값)를 구함\n","        lengths = abs(TFM_mean_l_norm - FM_k_l_norm)\n","        \n","        # ed를 취한 값들을 모두 더한 것이 se_lfmd임\n","        ab_lfmd = lengths.sum()\n","        \n","        return ab_lfmd\n","\n","    def fmd(self, FM_k):\n","        # 피처 맵 거리를 0으로 초기화\n","        fmd=0\n","        # lfmds: 레이어 피처 맵 거리를 담는 곳\n","        lfmds=[]\n","        # 각 레이어에 대한 레이어 피처 맵 거리 계산법으로 레이어 피처 맵 계산\n","        for l in range(self.L):\n","            FM_k_l = FM_k[l]\n","            # se_lfmd의 percent 인자의 디폴트 값이 50인데 이 다르게 하여 계산할 수도 있음\n","            lfmd_l = self.se_lfmd(FM_k_l, l)\n","            lfmds.append(lfmd_l)\n","        # 레이어 피처 맵마다 weight를 줌\n","        for l in range(self.L):\n","            fmd += self.W[l]*lfmds[l]\n","\n","        return fmd\n","    \n","    def set_fmdc(self):\n","        # fmds는 오분류 테스트 데이터에 대한 fmd를 저장함\n","        fmds=[]\n","\n","        # WFM을 부르기 위한 변수들을 선언함\n","        wtest=self.origin_names[2]; wtest_dir=self.wtest_dir; wtest_K=self.origin_K[wtest]\n","        WFMP_k=None; prev_WFMPI_k=None; cur_WFMPI_k=None;\n","\n","        # k(=0)가 속한 WFMP을 가지고 옴\n","        k = 0\n","        prev_WFMPI_k = k // 1000\n","        with open(f'{wtest_dir}/{wtest}_{prev_WFMPI_k}.pickle', 'rb') as f:\n","            WFMP_k = pickle.load(f)\n","\n","        # 오분류 데이터의 fmd의 최소값을 fmdc로 정함\n","        for k in range(wtest_K):\n","            # k번 째 데이터의 WFMPI, WFMPO 구함\n","            cur_WFMPI_k = k // 1000\n","            WFMPO_k = k % 1000\n","\n","            # WFMP_k가 이미 있다면 가지고 오지 않고\n","            #             없다면 이전 WFMP를 지우고 현재 WFMP를 가지고 온다.\n","\n","            # cur_WFMPI_k와 prev_WFMPI_k가 같다면\n","            if cur_WFMPI_k == prev_WFMPI_k:\n","                pass # 아무 작업 하지 않고\n","\n","            # cur_WFMPI_k와 prev_WFMPI_k가 다를 경우\n","            else:\n","                # 이전 WFMP_k의 기억공간을 램에서 제거한 후\n","                del WFMP_k\n","                # cur_WFMPI_k를 현재 WFMP_k를 가지고 온다.\n","                with open(f'{wtest_dir}/{wtest}_{cur_WFMPI_k}.pickle', 'rb') as f:\n","                    WFMP_k = pickle.load(f)\n","\n","            # prev_WFMPI_k를 cur_WFMPI_k로 초기화 \n","            prev_WFMPI_k = cur_WFMPI_k\n","\n","            WFM_k = WFMP_k[WFMPO_k]\n","\n","            fmds.append(self.fmd(WFM_k))\n","            \n","        fmds = np.array(fmds)\n","        \n","        self.fmdc = fmds.min()\n","\n","    def eval(self):\n","        for eval_name in self.eval_names:\n","            # is_ts_fmds에 각 데이터마다 ts_fmd이라면 1, 아니라면 0이 저장됨\n","            is_ts_fmds = []\n","            fmds = []\n","\n","            # EFM을 부르기 위한 변수들을 선언함\n","            eval_dir=f'{self.eval_dir}/{eval_name}'; eval_K=self.eval_K[eval_name]\n","            EFMP_k=None; prev_EFMPI_k=None; cur_EFMPI_k=None;\n","\n","            # k(=0)가 속한 EFMP을 가지고 옴\n","            k = 0\n","            prev_EFMPI_k = k // 1000\n","            \n","            # {eval_name}_{prev_EFMPI_k}.pickle이 있는지 확인\n","            is_there_eval_name_prev_EFMPI_k_pickle = os.path.isfile(f'{eval_dir}/{eval_name}_{prev_EFMPI_k}.pickle')\n","            # {eval_name}_{prev_EFMPI_k}.pickle가 있다면 이 파일을 가지고 오고\n","            if is_there_eval_name_prev_EFMPI_k_pickle:\n","                with open(f'{eval_dir}/{eval_name}_{prev_EFMPI_k}.pickle', 'rb') as f:\n","                    EFMP_k = pickle.load(f)\n","            # {eval_name}_{prev_EFMPI_k}.pickle가 없다면\n","            # eval_{prev_EFMPI_k}.pickle 파일을 가지고 온다.\n","            else:\n","                with open(f'{eval_dir}/eval_{prev_EFMPI_k}.pickle', 'rb') as f:\n","                    EFMP_k = pickle.load(f)\n","\n","            for k in range(1, eval_K):\n","                # k번 째 데이터의 EFMPI, EFMPO 구함\n","                cur_EFMPI_k = k // 1000\n","                EFMPO_k = k % 1000\n","\n","                # EFMP_k가 이미 있다면 가지고 오지 않고\n","                #             없다면 이전 EFMP를 지우고 현재 EFMP를 가지고 온다.\n","\n","                # cur_EFMPI_k와 prev_EFMPI_k가 같다면\n","                if cur_EFMPI_k == prev_EFMPI_k:\n","                    pass # 아무 작업 하지 않고\n","\n","                # cur_EFMPI_k와 prev_EFMPI_k가 다를 경우\n","                else:\n","                    # 이전 EFMP_k의 기억공간을 램에서 제거한 후\n","                    del EFMP_k\n","\n","                    # cur_EFMPI_k를 현재 EFMP_k를 가지고 온다.\n","\n","                    # {eval_name}_{cur_EFMPI_k}.pickle이 있는지 확인\n","                    is_there_eval_name_cur_EFMPI_k_pickle = os.path.isfile(f'{eval_dir}/{eval_name}_{cur_EFMPI_k}.pickle')\n","                    # {eval_name}_{cur_EFMPI_k}.pickle가 있다면 이 파일을 가지고 오고\n","                    if is_there_eval_name_cur_EFMPI_k_pickle:\n","                        with open(f'{eval_dir}/{eval_name}_{cur_EFMPI_k}.pickle', 'rb') as f:\n","                            EFMP_k = pickle.load(f)\n","                    # {eval_name}_{cur_EFMPI_k}.pickle가 없다면\n","                    # eval_{cur_EFMPI_k}.pickle 파일을 가지고 온다.\n","                    else:\n","                        with open(f'{eval_dir}/eval_{cur_EFMPI_k}.pickle', 'rb') as f:\n","                            EFMP_k = pickle.load(f)\n","\n","                # prev_EFMPI_k를 cur_EFMPI_k로 초기화 \n","                prev_EFMPI_k = cur_EFMPI_k\n","\n","                EFM_k = EFMP_k[EFMPO_k]\n","                \n","                fmd = self.fmd(EFM_k)\n","\n","                fmds.append(fmd)\n","                is_ts_fmds.append(fmd >= self.fmdc)\n","            \n","            fmds = np.array(fmds)\n","            is_ts_fmds = np.array(is_ts_fmds)\n","\n","            # is_ts_fmds: 각 데이터가 ts_fmd인지 아닌지 알려주는 넘파이 배열\n","            np.save(f\"{eval_dir}/{eval_name}_is_ts_fmds.npy\",is_ts_fmds)\n","            # fmds: 각 데이터의 fmd를 저장하는 넘파이 배열\n","            np.save(f\"{eval_dir}/{eval_name}_fmds.npy\",fmds)\n","        \n","    def show_all(self):\n","        print('self.show_data_infos')\n","        self.show_data_infos()\n","        print('self.show_FM_means')\n","        self.show_FM_means()\n","        print('self.show_AMs_and_related')\n","        self.show_AMs_and_related()\n","        print('self.show_hyper_parameter')\n","        self.show_hyper_parameter()\n","        print('self.show_index_infos')\n","        '''\n","        사용하지 않음\n","        self.show_index_infos()\n","        print('self.show_layer_infos')\n","        '''\n","        self.show_layer_infos()\n","        print('self.show_fmdc')\n","        self.show_fmdc()\n","        print('self.show_dirs')\n","        self.show_dirs()\n","        print('self.show_eval_infos')\n","        self.show_eval_infos()\n","        \n","    def show_data_infos(self):\n","        print(\"self.origin_names\"); print(self.origin_names)\n","        print(\"self.origin_K\"); print(self.origin_K)\n","        print(\"self.eval_names\"); print(self.eval_names)\n","        print(\"self.eval_K\"); print(self.eval_K)\n","        print(\"self.L\"); print(self.L)\n","        print(\"self.shape\"); print(self.shape)\n","\n","    def show_FM_means(self):\n","        print(\"self.TFM_mean\"); self.show_square_FM(self.TFM_mean)\n","        print(\"self.RFM_mean\"); self.show_square_FM(self.RFM_mean)\n","        print(\"self.WFM_mean\"); self.show_square_FM(self.WFM_mean)\n","\n","    def show_AMs_and_related(self):\n","        print(\"self.TAM\"); self.show_square_FM(self.TAM)\n","        print(\"self.RAM\"); self.show_square_FM(self.RAM)\n","        print(\"self.WAM\"); self.show_square_FM(self.WAM)\n","\n","        print(\"self.alpha_slice\"); print(self.alpha_slice)\n","        print(\"self.alpha_min\"); print(self.alpha_min)\n","        print(\"self.alpha\"); print(self.alpha)\n","        print(\"self.alpha_max\"); print(self.alpha_max)\n","        print(\"self.w_minus_r_max\"); print(self.w_minus_r_max)\n","\n","        print(\"self.DAM_select\"); print(self.DAM_select)\n","        print(\"self.DAM\"); self.show_square_FM(self.DAM)\n","        print(\"self.DAM_indexes\"); print(self.DAM_indexes)\n","\n","    def show_hyper_parameter(self):\n","        print(\"self.alpha_slice\"); print(self.alpha_slice)\n","        print(\"self.DAM_select\"); print(self.DAM_select)\n","        print(\"self.lfmd_select\"); print(self.lfmd_select)\n","        print(\"self.W\"); print(self.W)\n","\n","    def show_index_infos(self):\n","        print(\"self.mid\"); self.show_square_FM(self.mid)\n","        print(\"self.mean\"); self.show_square_FM(self.mean)\n","        print(\"self.std\"); self.show_square_FM(self.std)\n","        print(\"self.min\"); self.show_square_FM(self.min)\n","        print(\"self.max\"); self.show_square_FM(self.max)\n","\n","    def show_layer_infos(self):\n","        print(\"self.lfmd_select\"); print(self.lfmd_select)\n","        print(\"self.W\"); print(self.W)\n","\n","    def show_fmdc(self):\n","        print(\"self.fmdc\"); print(self.fmdc)\n","\n","    def show_dirs(self):\n","        print(\"self.root_dir\"); print(self.root_dir)\n","        \n","        print(\"self.origin_dir\"); print(self.origin_dir)\n","        print(\"self.train_dir\"); print(self.train_dir)\n","        print(\"self.rtest_dir\"); print(self.rtest_dir)\n","        print(\"self.wtest_dir\"); print(self.wtest_dir)\n","\n","        print(\"self.eval_dir\"); print(self.eval_dir)\n","\n","    def show_eval_infos(self):\n","        for eval_name in self.eval_names:\n","            print(f'In eval/{eval_name}')\n","            \n","            result_npy = np.load(f'{self.eval_dir}/{eval_name}/{eval_name}_result.npy')\n","            print(f'{eval_name}_result.npy'); print(result_npy)\n","            \n","            is_ts_fmds_npy = np.load(f'{self.eval_dir}/{eval_name}/{eval_name}_is_ts_fmds.npy')\n","            print(f'{eval_name}_is_ts_fmds.npy'); print(is_ts_fmds_npy)\n","            \n","            fmds_npy = np.load(f'{self.eval_dir}/{eval_name}/{eval_name}_fmds.npy')\n","            print(f'{eval_name}_fmds.npy'); print(fmds_npy)\n","    \n","    def show_square_FM(self, np_arr):\n","        # np_arr에 np_arr의 최소의 값부터 최대 값까지 그리는 넘파이 추가\n","        # 이 넘파이 배열을 그리는 이유는 최소 최대에 대한 시각적인 표현을 할 수 있을 뿐만 아니라\n","        # color bar로 인해 좁게 그려지는 넘파이 배열을 없앨 수 있다.\n","        # np_arr_min 찾기\n","        np_arr_min = np_arr[0].min()\n","        for i in range(1, len(np_arr)):\n","            if np_arr_min > np_arr[i].min():\n","                np_arr_min = np_arr[i].min()\n","        # np_arr_max 찾기\n","        np_arr_max = np_arr[0].max()\n","        for i in range(1, len(np_arr)):\n","            if np_arr_max > np_arr[i].max():\n","                np_arr_max = np_arr[i].max()\n","        # bool 타입이라면 마지막에 넘파이에 True, False만 넣고 즉, (T,T), (T,F), (F,T), (F,F)만 넣고\n","        if str(np_arr[0].dtype) == 'bool':\n","            np_arr.append(np.array([np_arr_min, np_arr_max]))\n","        # 그게 아니라면 마지막에 넘파이에 np_arr_slice로 잘린 연속적인 값들을 넣음.\n","        else:\n","            # np_arr_slice: np_arr_min 부터 np_arr_max 까지 그리는데 몇 번에 걸쳐서 그릴지 정하기\n","            np_arr_slice = 1000\n","            np_arr_interval = (np_arr_max - np_arr_min) / np_arr_slice\n","            # np_arr에 np_arr의 최소의 값부터 최대 값까지 그리는 넘파이 생성\n","            np_min_to_max = np.array([np_arr_min + i*np_arr_interval for i in range(np_arr_slice+1)])\n","            np_arr.append(np_min_to_max)\n","\n","        # 레이어 개수 만큼 레이어 원소 개수 계산\n","        element_count=[]\n","        for ith in range(len(np_arr)):\n","            ith_element_count = 1\n","            for ith_shape_ele in np_arr[ith].shape:\n","                ith_element_count *= ith_shape_ele\n","            \n","            element_count.append(ith_element_count)\n","        # 레이어 원소 개수로 정방 이차 배열의 한 변의 길이 구함\n","        square_FM_side=[]\n","        for ith in range(len(np_arr)):\n","            square_FM_side.append(math.ceil(math.sqrt((element_count[ith]))))\n","        # square_FM_side_min으로 그래프 크기를 정함\n","        square_FM_side_min = np.array(square_FM_side).min()\n","        square_FM_side_max = np.array(square_FM_side).max()\n","\n","        # 레이어 피처 맵을 평탄화함.\n","        np_arr_flatten=[]\n","        for ith in range(len(np_arr)):\n","            np_arr_flatten.append(np_arr[ith].flatten())\n","\n","        # x,y로 np_arr를 그리기 위한 이차 정방 배열 좌표를 만듦\n","        x=[];y=[]\n","        for ith in range(len(np_arr)):\n","            x_ith=[]; y_ith=[]\n","            for y_ in range(square_FM_side[ith]):\n","                for x_ in range(square_FM_side[ith]):\n","                    x_ith.append(x_)\n","                    y_ith.append(y_)\n","            # x_ith, y_ith를 np_arr[ith] 개수 만큼 자름\n","            x_ith = x_ith[:element_count[ith]]; y_ith = y_ith[:element_count[ith]]\n","            # x_ith을 x에 넣고 y_ith을 y에 넣음\n","            x.append(x_ith); y.append(y_ith)\n","\n","        # plt의 subplot의 크기 지정\n","        plt_column = 15\n","        plt_row = (len(np_arr)-1 // plt_column) + 1\n","\n","        # 넘파이 배열을 5줄 씩 그리기\n","        plt.figure(figsize=(3*20, 8*20))\n","        for i in range(len(np_arr)):\n","            plt.subplot(plt_row,plt_column,i+1)\n","            plt.scatter(x=x[i], y=y[i], c=np_arr_flatten[i], cmap='jet')\n","        plt.colorbar()\n","\n","    def save(self, model_name):\n","        # 인자로 지정된 경로와 이름으로 파일 저장\n","        with open(f\"./instances/{model_name}.pickle\", \"wb\") as f:\n","            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n","\n","    def load(self, model_name):\n","        # 인자로 지정된 경로와 이름으로 파일 불러오기\n","        with open(f\"./instances/{model_name}.pickle\", \"rb\" ) as f:\n","            return copy.deepcopy(pickle.load(f))"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-21T12:39:14.056779Z","start_time":"2022-07-21T12:38:44.286277Z"},"id":"W7kccVRJzWBX","scrolled":true},"outputs":[],"source":["fmd1 = FMD(\"/Volumes/My Passport_ssd_sg3/data_sets/fmnist/shirts\")\n","# fmd1.create_practice()\n","fmd1.fit()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-21T12:39:14.069993Z","start_time":"2022-07-21T12:39:14.061277Z"},"id":"y80256ulT3NH","outputId":"f4bf6feb-96e6-4a6d-cf47-468dbc6feaca"},"outputs":[{"name":"stdout","output_type":"stream","text":["self.fmdc\n","4199.252005996141\n"]}],"source":["fmd1.show_fmdc()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-21T12:41:21.240892Z","start_time":"2022-07-21T12:39:14.076161Z"},"id":"w16Fbn0fj5Wp","scrolled":true},"outputs":[],"source":["fmd1.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-21T12:41:25.037337Z","start_time":"2022-07-21T12:41:21.259783Z"},"id":"A8Rsu4zdj5Wq","outputId":"f587c679-60a0-452b-92fe-947089cc9d39"},"outputs":[{"name":"stdout","output_type":"stream","text":["In eval/rotation_90\n","rotation_90_result.npy\n","[0. 0. 0. ... 0. 0. 1.]\n","rotation_90_is_ts_fmds.npy\n","[ True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True False  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True False  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True]\n","rotation_90_fmds.npy\n","[4586.88271782 4225.62244068 4498.08742087 4887.49567088 4777.08080735\n"," 4628.92347242 4802.13800927 5736.60086962 5052.1686049  4722.30701888\n"," 5018.36788627 4724.17481065 4838.88744912 5050.53881259 4831.0962998\n"," 4495.16715402 4661.42279479 4525.27124635 4724.27842117 5605.6109501\n"," 4629.92514663 4843.71327101 4895.31007833 4886.47339446 4519.60620308\n"," 4898.60447892 4664.23954158 4867.29145728 4672.87101192 4670.62656943\n"," 4926.61529164 4529.19326834 4806.69828729 5405.04794683 4934.828869\n"," 4975.58301244 4881.29694478 6035.98553741 6059.56130963 4496.00184955\n"," 4766.34583452 4933.61597287 5055.24648349 5895.66152133 5306.23026776\n"," 5010.48709305 4753.42816835 4412.41747088 4764.90633735 4875.23853268\n"," 5981.42259354 4719.26582969 4477.97321421 4345.5213623  4608.01033897\n"," 4952.13198102 4410.1694466  4439.33069335 4780.779776   4986.80526708\n"," 5798.43194362 4632.98943628 4587.81567956 4629.69124422 4742.56332152\n"," 4562.67557551 4840.67115933 4484.72223841 4874.38969574 4644.59097867\n"," 5856.90327077 4979.20965412 5019.96896306 4401.44258195 4427.2536561\n"," 4501.39381861 4592.879666   4622.79464355 4396.20009281 4805.45596431\n"," 5773.48582476 4986.10752365 4909.04814484 4598.45546521 4763.71623878\n"," 4614.37295635 4770.39943384 4644.21224831 4742.40063005 5827.73949588\n"," 4902.55306884 4364.48282024 4400.0101956  4277.4785956  4466.3912221\n"," 4903.01861295 4544.65731393 4979.46322893 4964.23558162 4897.29626309\n"," 4760.72621495 4558.18623184 4945.94203532 4775.85015652 4750.67492719\n"," 4559.0469138  4544.38918516 4828.90688047 4305.2504868  4627.94175356\n"," 5020.70838237 4511.46542345 5016.53153596 4800.32534556 4327.45747724\n"," 4875.89610841 4568.97708706 4485.10229912 4821.57989445 4400.24946626\n"," 4854.30044479 4620.27647121 4786.06417338 4716.1466246  4915.07641818\n"," 4942.24178992 4698.05754506 5077.94093093 4828.28350848 5360.31478502\n"," 4947.26383556 5730.55236076 5450.70978212 4805.8115629  4334.46201461\n"," 4456.12117252 4452.41288829 4674.19899    4584.09931842 5468.02064071\n"," 4807.22936734 4908.8400641  4685.87816125 4854.14959875 4906.30613767\n"," 4655.53354079 4664.31496257 4763.01415773 4887.64365328 4792.149007\n"," 5007.10733783 4398.93319344 4853.81178272 4455.70924306 4736.85293193\n"," 5169.04231046 4632.13043792 4416.19671139 4582.0235312  4859.74001864\n"," 4512.53223456 5798.43194362 4922.68781661 4910.59555689 4932.70319124\n"," 4492.98746896 4853.61175988 4395.24188789 4479.10076162 4348.46262346\n"," 4626.46706883 4469.20132664 4413.13611514 4663.09740107 4934.6561017\n"," 4935.84977621 4769.35424934 4488.33577086 4682.82195426 4418.91996161\n"," 5988.55448179 5013.17424421 4761.67217564 4870.80575775 4775.40291626\n"," 4497.06597865 4918.23697404 4835.16774105 4533.33986076 4516.15968439\n"," 4866.74900903 4776.92103603 4818.33678479 4691.26832833 4859.91665743\n"," 4929.04174368 4789.11901545 4900.33517401 4990.12539398 4970.42015397\n"," 4478.71797473 4947.63891713 4476.73276295 4427.84293411 4800.5895102\n"," 4550.87543388 4224.6404204  4810.39967649 4585.98461931 4698.74282284\n"," 4922.63335955 4671.72553501 4931.55226164 4352.6991563  4494.09477479\n"," 4384.23435189 4644.58893989 4829.85315354 4960.86541285 4905.40542649\n"," 5679.2423159  4324.02607062 4897.04342601 4339.22748652 4809.62317023\n"," 4207.5246586  5287.04649412 4689.06075814 4897.19934736 4902.94923412\n"," 4684.46363945 4821.2170153  4502.27211001 4795.57856676 4451.55225208\n"," 4651.99892977 4648.80463254 4995.67040536 4815.30707521 4454.29136123\n"," 4785.25285411 5501.02615815 4769.19565159 4523.67753349 4294.15276644\n"," 4817.79603641 4908.54546227 4916.02942246 4748.38503329 4592.23005577\n"," 4936.95219076 5092.10395423 4783.69548048 5065.28672528 4979.84193523\n"," 4811.35492049 5937.94149966 4472.72410567 5400.53070698 4841.19712901\n"," 4866.96688258 4345.18098237 6021.37348855 4897.39206582 4884.27407129\n"," 5469.33574227 4603.9241129  4400.11844868 4767.45359536 4880.26373524\n"," 4572.28481965 4585.26936318 4866.71752827 5009.2231545  4472.75521046\n"," 5178.31267    4778.5465594  4820.3406257  4765.91715772 5703.47030655\n"," 4670.36626196 4242.79230863 4865.66586503 4801.09051417 4643.81561843\n"," 4447.40493751 4684.21874211 4450.75653057 4684.0082653  5984.14951098\n"," 4276.34924504 4921.51139015 4882.92067982 5719.70301718 4920.47750719\n"," 4428.23533884 4954.37296976 5402.29885823 4299.02613939 4797.05911217\n"," 4733.73743853 4713.30985099 4284.00973162 4254.23307392 4825.25850838\n"," 4353.18100189 5094.316659   4457.29206347 4920.07710547 4951.5297426\n"," 5729.84813618 4690.15097741 4839.6988287  4550.67292166 4307.36805537\n"," 4910.1884716  4892.19744309 4909.01051363 4494.53622421 4914.76171291\n"," 4487.35836443 4688.83083057 4568.01804157 4474.61599868 4615.20824686\n"," 4826.58401077 4725.45611939 4708.12778728 5821.90847777 5168.96644778\n"," 4973.92302641 4824.7741084  4894.43093193 5101.81654844 4527.27719135\n"," 4863.52199888 5264.47653383 4346.30197213 5009.50596328 4481.07491621\n"," 5554.42084007 4944.68180176 4446.27118475 4705.03360786 4910.72930612\n"," 4827.87176483 4566.06339173 4841.28364169 4899.7442639  6057.00992347\n"," 4733.59482229 5398.07481823 4668.52753227 4433.01315326 4589.50168961\n"," 4471.88268614 5068.16970253 4775.8453404  5039.64214224 4865.87567274\n"," 4477.77312545 4859.11137161 4534.20342358 4886.48247013 4798.32934596\n"," 4736.27336986 4858.0484384  4763.21492207 4772.02682761 5932.73443257\n"," 4904.91236402 5925.04974458 4456.46767114 4676.89910134 4286.62875138\n"," 4488.58001513 4743.01109633 4446.33891496 4637.98494287 4781.66771172\n"," 4596.07125658 4439.35572974 5721.60490604 4491.95015954 4793.09070898\n"," 4959.69034084 4701.30932969 4924.75005898 5915.30639689 5789.59227967\n"," 4822.90551214 4221.50340536 4943.78329031 4514.30518522 5024.08650726\n"," 4875.10347113 4749.82076662 4848.41479342 4922.87112261 5042.61804492\n"," 4752.58149663 4849.7003342  4643.90639667 4434.98494955 4742.19583285\n"," 4819.69034816 5928.71745343 4422.93337334 5851.12976282 4915.40619307\n"," 4785.74940852 4410.3501232  4788.15267128 4680.04593477 4703.72741907\n"," 4230.01561959 4762.73752862 4533.8070057  4870.22475104 4344.97037334\n"," 4719.07809957 4743.36701863 4507.82814949 5872.67479251 4557.07508191\n"," 4959.32920762 5074.2819531  4731.95270788 4925.97479458 4935.79576409\n"," 5003.73607771 4459.99699402 4564.7261723  5035.61620714 4707.3134814\n"," 4862.08049303 5937.78181435 4704.2044411  4667.2199892  4384.22936649\n"," 5874.46201329 4797.40642186 4950.86997155 5616.5605025  4499.25565317\n"," 4764.08322173 5963.48346007 4657.97498347 4724.85553493 4289.70244836\n"," 4758.24735922 4746.7121392  5804.6498768  6025.45329696 4969.68755475\n"," 4791.22270112 4932.51453155 5847.64156249 5740.96656783 4703.03898457\n"," 4956.81175144 4944.14744507 4725.80693066 4901.69308073 4610.63590217\n"," 4988.30373228 4557.03371578 4694.05477954 5837.52953828 5715.23225722\n"," 4783.62963897 4978.25221733 4374.34654741 4921.87621682 5579.1173771\n"," 4902.56925713 4589.93866861 4913.92705615 4714.89145367 4852.67740757\n"," 4272.89122281 4931.58006923 4523.43763537 5905.00331575 4910.99448818\n"," 4580.18650527 4938.26536702 4864.74076625 4434.39909691 4784.03126637\n"," 4430.66269766 4747.85502266 4407.56053992 4841.10197171 4744.72867011\n"," 4593.69201083 4297.36031305 4483.26904806 4792.04171228 4677.15305567\n"," 5140.39829485 4353.93759954 4738.06873947 4947.32550795 4674.00782976\n"," 4568.40008978 4642.56713749 4479.91095352 4814.15041407 4987.8286251\n"," 4684.36883266 4177.47054459 4839.21839945 4883.88660087 4916.10429877\n"," 4788.77067575 4784.34859242 4924.48423091 4624.306652   4832.06812579\n"," 4962.40498354 4982.11643128 4710.48994059 4706.34748833 4793.12276299\n"," 5350.33480814 4548.517417   4577.63223678 4935.93500886 4696.91192497\n"," 4471.56894184 4338.25465949 4952.33012203 4482.43430705 5600.39967185\n"," 4900.02561623 4486.46325963 4867.32266387 4907.66521092 4951.67992599\n"," 4535.32398205 5204.07990259 4617.31421951 4430.50060538 4558.05963662\n"," 4914.70049629 4866.27123652 4915.92932475 4716.44888708 5041.67891683\n"," 5586.41037977 4786.13222983 4450.38041898 4406.05388749 4408.60650714\n"," 4802.61144642 5914.76158401 5977.58460225 5906.9487806  4519.82328313\n"," 5028.01604464 4861.18856648 4404.61528781 4985.74015235 4992.11223756\n"," 4588.49725005 4893.36878684 4845.76389302 4761.2312243  4507.95667457\n"," 4973.131682   4863.02790929 5753.40673058 4838.74900011 5668.79150467\n"," 4766.34583452 4566.53460105 4427.27516639 4732.43581197 4698.99817623\n"," 4561.28636097 4950.29518248 4536.1441052  4845.27422272 4750.00118644\n"," 5413.07653779 4881.83063895 4875.89343075 4305.21875036 4875.53226879\n"," 4556.95768352 4926.04671264 4994.35791561 4205.01440421 4648.46795194\n"," 4880.31977333 5004.23290549 4210.39639302 4560.80675    4454.54911987\n"," 4962.56646611 5707.18388778 4761.55917657 5931.39634433 4813.39977849\n"," 4823.49395222 4363.76536786 4453.06687282 4831.89709645 4352.30298029\n"," 4665.06713022 4525.54489696 4804.87382365 5850.09167126 4623.96867824\n"," 4550.4271873  5754.69953872 4754.91041827 5427.17815534 4700.16745515\n"," 5077.00761772 4511.87335725 4590.73124065 4855.70925886 4936.62159888\n"," 4540.67474148 4581.16425217 4562.53613552 4949.33239158 4938.45100693\n"," 4920.47750719 4508.94998438 4939.7865352  5931.39634433 4880.84556827\n"," 4723.57442038 5050.08717355 4699.8162954  4565.00607182 4579.54843082\n"," 4742.29651518 4605.3012828  4581.76414588 4378.49347057 4935.57985358\n"," 5030.29285697 5952.92220018 5005.43571452 4218.0463495  4595.54935073\n"," 4706.99642847 5065.28672528 4791.99315301 4742.75664327 4956.04331744\n"," 4663.1022721  4977.08529494 5015.27005932 4262.77950548 5853.75711767\n"," 4984.78985838 4955.53853353 4890.07663182 4789.11998207 4764.65103482\n"," 4893.13599383 4934.37956541 4741.69014271 5007.34500673 6039.90339269\n"," 5597.12630502 4600.25216827 4593.36070005 4967.6253425  4778.07715347\n"," 4822.63805826 4708.27919225 4785.01708432 5457.99526306 4826.19793886\n"," 4872.49294492 4533.02657394 4535.27438224 4604.62480038 5845.12644288\n"," 4737.52736068 4572.17549049 4990.90109309 4509.34989263 4287.0811486\n"," 4663.20898795 4843.88498733 4760.91508607 4722.33395234 4318.38970735\n"," 4906.21640586 5922.28033201 4460.18240791 4714.76041268 4871.36156904\n"," 4879.16571113 4939.88743023 4948.90024463 5580.75812126 4893.12937037\n"," 4480.60912842 4496.61816561 4768.34501167 4954.55996239 4896.0578257\n"," 5671.42569071 5007.48131996 4733.08310001 4825.78933659 4482.70607721\n"," 4797.02780232 5808.12291314 4580.6865793  4836.30072796 4805.56129588\n"," 4929.2635183  4983.05934412 4505.04703647 4831.15895917 4905.92264235\n"," 4638.0644678  4788.62224011 4704.97995339 4755.44703875 4484.60392535\n"," 4482.74257206 4944.86052265 4681.19970833 4892.8783458  4602.42619766\n"," 4613.9597397  4663.48031426 4690.07589979 5329.77776299 4644.80808223\n"," 6045.69336065 4637.13127    4719.05044795 4616.13242986 4862.18402571\n"," 4854.92893115 4437.06747158 4736.30138681 5638.99330471 4618.7509193\n"," 5264.47653383 4831.98326118 4949.84941779 4773.45461532 4530.04995139\n"," 4786.42159704 4911.73103676 4514.07079344 4942.13628052 4573.45161792\n"," 5861.03755231 4671.33902872 4444.99468409 4898.8997817  4556.46977978\n"," 4845.86241257 4666.81078659 4660.50482571 5316.25504285 5883.24076203\n"," 4305.08512561 4414.84818975 5730.55236076 4871.68468375 4714.51508558\n"," 4804.80436604 4785.16031671 4802.78836079 5850.27242158 4805.48759103\n"," 4512.07130528 5088.00583849 4899.59377676 4696.18205172 4495.75375307\n"," 4698.30077912 5020.74819098 4628.09005986 4605.7003583  4551.75802286\n"," 4580.98637197 4787.44446153 4769.64264857 4790.45719584 4558.80220131\n"," 4839.54389606 5745.30373456 4982.972376   4889.93473754 4744.43443203\n"," 4757.35008941 4922.77584673 4978.949466   4902.08060451 4657.07062384\n"," 4243.31455678 4966.4084606  4902.08060451 4961.45651254 4840.58873593\n"," 4884.53493796 4914.91831234 4464.98334167 4205.2451411  4912.46532051\n"," 4308.08277103 4860.89314914 4892.38696833 4639.28793283 4840.05119591\n"," 4863.24094769 5873.14304595 4436.72805616 4483.83783553 4909.77577943\n"," 4818.87998941 4707.60293561 4595.65557823 4585.60029177 4491.59372015\n"," 5312.26544041 4682.63662515 4734.62377838 4582.89796945 5635.08526681\n"," 4726.02313167 4780.09025733 4727.73376252 4852.13851083 4906.26751456\n"," 5029.66209677 4533.57420631 4854.67994242 4790.30287301 6036.83892198\n"," 4730.68369341 4476.15442925 4711.14864824 4767.29335931 4261.94410028\n"," 4801.02240722 4929.68957195 4508.94998438 4651.61049876 4354.13792012\n"," 4454.29136123 4979.39620008 5490.56208013 4777.31484615 4862.75139838\n"," 4823.82579367 4561.5966184  4522.23404406 4824.13771506 4879.86290794\n"," 4869.88853991 4984.86703762 4906.51909455 4911.33485555 4841.19712901\n"," 5727.06203373 4820.3504475  4780.69551887 4908.03084306 4901.43498632\n"," 4769.37095714 4831.79611438 4888.678761   4862.42547616 4427.93063633\n"," 4444.71484148 4348.10256273 4554.00393384 5423.5788918  5675.78742201\n"," 4311.45067045 4791.17139457 4323.71151396 4358.71259333 4880.54614852\n"," 4705.59345358 4979.14147631 4360.45366916 4805.41896392 4342.73929036\n"," 4188.04819484 4885.83461729 4695.34063857 5980.37904542 4355.31344203\n"," 4592.5846383  5600.48738409 4890.28901717 4254.32465501 5037.17494758\n"," 4275.84627174 4827.19496097 4788.96112056 4453.66721279 4685.76444899\n"," 4612.912928   4713.3749842  4666.81078659 4577.65887337 5840.25422809\n"," 4663.62257326 4388.43226477 4505.02485841 4755.28112204 4614.89567446\n"," 4504.39994324 4445.24481443 4522.840913   4468.1132214  4777.00230978\n"," 4548.19869387 4761.50743346 4843.52982077 4870.12245106 4754.76285149\n"," 4644.58105497 4848.58167392 4825.55732781 4940.62178296 4437.12709906\n"," 4727.63722244 4938.93062419 4746.21127713 5960.82483189 4769.56571525\n"," 4763.23739142 4679.49007144 4922.89390128 4843.40558874 4369.04182121\n"," 4217.30536642 4748.60051014 5041.2408078  4909.98878036 5620.67404198\n"," 5117.0920866  5019.07117774 4437.44726686 4604.72288029 4675.77919227\n"," 4801.85101429 4919.44169885 4537.71857682 4804.95512891 5961.91128709\n"," 4987.98920474 4994.61809407 5686.46003278 4952.97305336 4924.81182147\n"," 4877.43312288 4506.2821442  4869.11247306 5982.84477921 4716.97184976\n"," 4843.08398745 4514.67832773 4521.80673211 4738.6098461  4678.10406072\n"," 4439.83257444 4933.88605938 4413.6878628  4799.16698043 4625.46538311\n"," 4463.32531057 4463.3912386  4749.26411343 4997.52668965 4631.77077122\n"," 4379.76392251 4647.55609793 4563.48504459 5058.8134868  4935.79838105\n"," 4461.41326682 6018.68635904 4314.68857039 4660.91445371 4325.51526124\n"," 4233.59378307 4343.15797009 4393.51621799 4727.28381722]\n","In eval/brightness_10\n"]},{"name":"stdout","output_type":"stream","text":["brightness_10_result.npy\n","[0. 0. 0. ... 0. 0. 1.]\n","brightness_10_is_ts_fmds.npy\n","[ True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True False  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True False  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True False  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True False  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True False  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True]\n","brightness_10_fmds.npy\n","[4741.7961069  4895.79815099 4744.66600359 4724.89926653 4506.33145262\n"," 4943.77616916 4479.14787314 4911.94510641 4736.04219241 4399.01887288\n"," 4950.17131085 4898.6774384  4725.20246722 4761.86104734 4901.90898304\n"," 4244.71604099 4886.65789561 4967.13054736 4273.10827715 4921.63579968\n"," 4922.40282905 4651.63519428 4537.67589507 4419.63519857 4347.9567297\n"," 4302.50981027 4909.56033275 4925.52237353 4742.36368367 4481.22969656\n"," 4760.27134447 4827.4739574  4909.40863475 4542.4013496  4832.88242552\n"," 4334.78892062 4873.57141871 5350.95740187 5889.41419199 4730.01179797\n"," 4677.91565304 4701.09944433 4422.08979499 4957.13776234 5921.88537246\n"," 4772.854605   4904.75727057 4893.37630733 4795.72785745 4783.07815062\n"," 4377.24424349 5737.16034673 4764.23629073 4870.75901061 4552.99372468\n"," 4467.55671322 4887.46963102 4863.22973793 5146.97440014 4601.24211727\n"," 4436.72805616 4563.12817209 4567.99342098 5017.02366456 4726.70573891\n"," 4401.40718712 4915.68042239 4710.70315136 4548.21182081 4738.51951385\n"," 4708.56763431 5637.67976048 4936.32268013 4493.38597847 6019.36468522\n"," 4459.4258511  4525.35409846 4491.52567406 6003.33743886 4336.40922564\n"," 4825.43710001 4818.03958318 4484.28690369 4387.33527352 4845.95022141\n"," 4804.34981984 4920.54891304 4952.36315097 4931.42070523 4712.59230859\n"," 4829.89383948 4840.43605454 4509.99738069 4956.19075116 4640.17435395\n"," 5657.06970727 4606.47267374 4444.6916136  4846.10915951 4694.18671705\n"," 5096.10732875 4958.12229858 5111.09652854 4829.75851702 4766.27960884\n"," 4851.37613571 4486.20948373 4854.96609376 4666.15073297 4371.06709517\n"," 4659.15018535 4739.62423811 4775.2453472  4823.809617   4872.45412298\n"," 4755.9326189  4405.57522544 4761.04069164 4776.95889405 4985.80331775\n"," 4919.22426656 4835.29602923 4743.59382478 4460.71004499 4928.47353233\n"," 4612.97426354 4523.55547829 4562.18199634 4691.52307729 4977.35575626\n"," 5833.46111329 4777.31484615 4844.59569223 5032.84043189 4721.53778245\n"," 4874.90283075 4567.62827798 4746.14033305 4737.4296768  4578.43081859\n"," 4900.91693407 5932.28997824 5857.66192045 4801.47088241 4850.60324707\n"," 5939.94776939 4411.41224838 4570.73529745 4696.75661619 4780.87624053\n"," 4233.19136142 4440.65429093 4661.74909563 5646.35111078 4776.36471251\n"," 4705.31596139 4761.09195762 4875.88289993 4622.5283865  5247.87989252\n"," 4653.32733044 4783.8944306  4762.72023144 4377.10697409 4770.00444677\n"," 4458.26200643 4841.28364169 4849.83147724 4719.17916021 4851.91746975\n"," 4838.12019105 4788.10140215 4690.87461911 4708.11507339 4806.03517614\n"," 4857.3962288  4387.54003069 4752.50506    4460.30316803 4609.07440108\n"," 4914.55794588 4952.96181079 4999.59858077 4591.21702377 4819.80799066\n"," 4295.67538817 4480.92892472 4968.76580324 4337.32755672 5693.93354552\n"," 4804.32748527 5922.43057958 4763.04476199 4400.72448347 4820.86469464\n"," 4310.64651986 4657.58132113 4407.61822876 4753.01435702 4823.67536258\n"," 4691.61667555 4860.56595542 5823.29778148 4630.38224394 4981.17003865\n"," 4792.27455845 4774.09523089 4626.13713336 4838.17152314 4889.47685734\n"," 4467.83548899 4542.26603529 4927.2515665  4760.41819131 4291.60955502\n"," 4848.51972663 4437.18844266 4771.1422883  4699.56442417 4781.55935734\n"," 4475.7500976  4222.51384861 4582.57696449 4257.85343487 4534.14040372\n"," 4498.77064017 4975.26178234 4478.70565414 4763.80519675 4734.09442697\n"," 5334.23579747 4541.54464304 4926.64627183 4457.87849178 4495.55074913\n"," 5877.19363762 4897.49063187 4744.7402593  4892.38696833 4836.08845851\n"," 5474.66858286 4715.67444763 4414.13320193 4771.25100897 5097.12462353\n"," 4695.07611018 4812.91426284 4989.06794409 4510.80654226 4744.94937683\n"," 4818.85677066 4796.23254342 4540.03883525 4291.64932939 4456.35790401\n"," 4629.82813906 4593.8571697  4850.75384158 4622.04185292 4970.66546417\n"," 4437.59301765 4701.94205697 4822.73244179 4729.0455327  4634.40112029\n"," 5032.52372351 5997.22064224 4405.77117115 4216.34229137 4929.4800242\n"," 4528.64976688 4689.09983667 4952.6266256  4611.84803431 4940.10876513\n"," 4809.44314181 4627.47764288 4203.98678019 4562.35615381 4718.2476671\n"," 4708.32227613 4429.92432851 4805.7382389  4794.25579049 4556.77085666\n"," 4488.87440926 4595.47373344 4659.90562483 4837.8305977  4877.43312288\n"," 4770.32049499 4741.78781975 4474.24037871 4844.13431953 5954.33232573\n"," 4851.12690391 4544.26786634 4752.96929499 4469.45041689 5216.52165964\n"," 4937.89996804 4699.09434794 4208.11309101 4746.57197489 4566.56542639\n"," 4718.27004395 4908.35472173 4369.54502636 4927.08623864 5916.22864443\n"," 4549.86657616 4249.53923472 4847.59031345 4313.02955309 4436.65387034\n"," 4520.36712923 4283.0439991  4563.11759994 4817.51198789 5015.71942437\n"," 4884.07268244 4782.14139031 4770.37502122 4624.09576221 4851.32835946\n"," 4859.98915758 5079.87658752 4933.88605938 4711.68118928 4558.81892759\n"," 4969.75885857 4333.893453   4441.97069083 4156.90344621 4738.801193\n"," 4966.44480508 5759.06506077 4568.60891721 4773.4538375  4683.9217811\n"," 4824.89845151 4736.87936356 4328.62822795 5595.08413402 4709.51601997\n"," 5010.63114302 4494.89537939 4723.93889184 4874.98342022 4844.65382116\n"," 4414.42737087 4278.02559963 4551.09946714 4748.38503329 4390.73978182\n"," 4861.55399113 4608.33482081 4581.37343444 4853.9390432  4219.13752122\n"," 4919.51923117 4817.77896472 5443.30058883 4724.4718761  4923.81907133\n"," 4806.10922877 4951.17924799 4802.26950622 5962.42479001 4533.32402341\n"," 4583.65630257 4423.31131529 4558.81395761 4653.08797948 4752.547542\n"," 4958.03115945 4611.74613247 4336.83463441 5366.51960948 4792.15555675\n"," 4533.81676117 5763.1767241  4961.89832241 4813.25525091 4607.09785449\n"," 4778.87640892 4830.72783664 4860.7052047  4482.92191634 4805.33728865\n"," 4905.06186671 4961.89832241 4756.54010366 4399.97258348 4822.64229406\n"," 4356.37774155 4966.9742105  4864.10634806 4620.03124392 4802.56725081\n"," 4708.25602276 4749.30509083 4946.00841541 4439.5749067  4867.26830306\n"," 4702.19038566 4924.42742455 4899.67948474 4774.55289751 5614.13820417\n"," 5843.06666436 4658.01171781 4456.35790401 4612.75438734 4216.34229137\n"," 4611.70773131 4762.52402508 4705.6382479  4707.93219605 4676.77596033\n"," 4854.53164135 4649.70785495 4745.04038262 5910.01230101 4899.67432263\n"," 4261.80765164 4389.98767892 4458.59424554 4980.36314264 4497.1372806\n"," 4938.24684156 4942.44811318 4796.94860761 4712.25042563 6008.09300395\n"," 4666.97695366 5811.60518791 4194.11823145 4467.55671322 4747.84032976\n"," 5659.99358827 4589.71637983 4360.67356047 4704.54149018 4843.31178085\n"," 4850.29001811 4901.18913258 4940.98498243 4967.20515891 4772.39565663\n"," 4885.34891588 4689.94453685 4504.60753167 4628.35865541 4938.91540921\n"," 4478.05692309 4406.41023094 4852.08477744 4779.98516881 4497.1372806\n"," 4807.22936734 4878.67168213 4266.99040586 4636.54673193 4220.20459151\n"," 5233.00226928 4685.67655052 4815.30707521 4437.75049693 4741.78781975\n"," 4729.51260535 4648.0991605  4700.16745515 4633.36573798 4949.81495441\n"," 4655.01619003 4194.60275835 4857.5108271  4528.28107979 5056.22418025\n"," 4587.34602672 4948.17608572 5843.92655085 5023.16418202 4732.87350367\n"," 4913.08922083 4705.99861856 4669.98617946 4572.60438984 5779.82698589\n"," 6012.22998321 4306.99186547 4255.76072178 5096.31022441 4893.89778439\n"," 4688.46296334 4339.7892533  4692.11213338 4897.48964382 4847.69909915\n"," 4987.38023826 4673.07716841 4409.26295042 4914.67323503 4814.12922674\n"," 4594.49427045 5021.72773897 4604.35666031 4699.05920245 4646.85239734\n"," 4815.94802085 4714.38224268 4388.8769843  4291.29285601 4692.68527413\n"," 5834.39066361 5643.38690479 4728.66015014 4925.87800853 4798.00337866\n"," 4622.72003693 5226.92895133 4243.82750858 4307.91561939 4827.764873\n"," 4831.81155868 4393.61929039 4809.23088547 4216.24301968 5997.22058714\n"," 5109.00711453 4818.33678479 4303.93733768 4803.14211374 4833.08465739\n"," 4471.32804651 4855.2567901  4234.72888766 4907.97621734 4377.06377352\n"," 4649.496984   4812.8333331  5603.29862061 4642.85962394 4640.34532033\n"," 4745.5904699  4659.08726107 4873.70459027 4914.12718351 4887.21186352\n"," 4601.31028045 4617.47812303 5171.19130598 4511.42008898 4322.67926643\n"," 4573.57171296 4721.50529615 4737.44362973 4659.43757677 4978.45640006\n"," 4661.22203378 4969.35501403 4764.31295437 4788.30696375 4949.86115413\n"," 4591.13446514 4561.02992127 5541.53441873 4758.35798089 5852.98303436\n"," 4723.74710719 4634.40510507 4865.86709715 4427.51014103 4669.6902585\n"," 4785.92116501 4728.81227357 4857.25123438 4749.35413256 4718.53082557\n"," 4904.61597719 4643.92531239 4718.19984398 4759.69076599 4989.33257234\n"," 4779.6625031  4252.36331757 4893.84734554 4917.17018381 4735.70032639\n"," 4807.14342406 4455.50716868 4737.53353979 4966.94812224 4626.03752239\n"," 4716.1466246  4780.00999091 4534.67395186 4577.20062906 4730.90532713\n"," 4681.94708397 4680.39657631 4302.40682547 4738.54606936 4781.91654341\n"," 6016.24322984 4867.27636015 4305.21875036 4623.48623547 4674.63085867\n"," 4704.56032644 4721.70623359 5018.68257278 4847.29450856 4654.76676426\n"," 4581.48513533 4821.9414846  5959.14546778 4913.70118002 4555.37947078\n"," 4362.97225678 4789.74681048 4726.46620168 5001.0578521  4719.6040483\n"," 4894.85863606 4500.01283303 4776.77708395 4760.78777275 4849.06073122\n"," 4772.89632463 4435.82046663 4625.23959091 4765.18850712 4848.18054214\n"," 4902.55306884 5086.6614136  4641.58543238 4790.32022567 4700.6741673\n"," 4825.85167111 4566.20086392 4887.06239996 4289.66361372 4640.02722422\n"," 4657.92686063 4726.69297361 4664.51620527 4693.79982028 4939.45400023\n"," 6028.54515111 4708.46713051 4724.137659   4855.94183623 4908.48440278\n"," 4738.40824011 4545.65949853 4305.08512561 4750.6040255  4490.98896456\n"," 4874.09026347 4627.03417653 4460.80724283 4472.388515   4872.07547991\n"," 4887.72983307 4448.13734602 4507.31563128 4353.23382989 4559.94386943\n"," 4693.46121422 4730.81008929 4841.36198127 4529.3495223  4743.25995055\n"," 5046.19062591 4458.74747703 4751.76453096 5061.21413268 5914.76158401\n"," 4668.80532716 4826.90227241 5808.12291314 4621.10211658 4707.56212119\n"," 4493.4954471  4764.9769299  4746.59547525 4873.53145302 4217.66290121\n"," 5475.66429599 5531.15801501 5005.75309157 4716.10044654 4253.65794145\n"," 4978.7981771  4966.12513339 4404.14093076 5057.06172347 4438.46577562\n"," 4782.72103052 4668.8400936  4953.87207332 4406.24247887 4837.28260524\n"," 4727.25975956 4743.63120114 4472.13829597 4724.48848397 4996.29439666\n"," 4400.8366216  4483.28782086 5411.69525721 4951.24915183 4383.69603739\n"," 4933.76542721 4970.14395309 4943.80572271 4550.77040656 4784.81286115\n"," 4566.10952944 4377.05512709 4385.53101967 4959.66082072 4856.26869308\n"," 4721.02417211 6011.1769234  4441.4270715  4573.52171387 4920.77866385\n"," 4851.18164333 4245.32342721 4734.84946886 4774.07976207 4852.02302291\n"," 4893.33390134 4404.46312282 4862.266084   4677.88870271 5011.75001263\n"," 4777.57644244 5186.56249018 4570.28253174 4751.31965069 4929.27005244\n"," 4910.76331847 4406.6403743  5026.76906945 4581.70503311 4609.13583596\n"," 4423.08238765 4664.81461591 5131.64147708 4598.99320914 5069.66462839\n"," 4748.51237758 4414.28561278 4954.21147175 4568.03172495 4223.86051672\n"," 4487.0787077  4238.29328141 4414.0164885  4802.50119005 4726.13629454\n"," 4408.68662814 4719.08658118 4928.22792358 4814.70016405 5150.61877648\n"," 4420.35721404 5033.43938505 4908.87519434 5382.8852892  4557.90460497\n"," 4958.32391703 4962.6385358  4977.02348217 4730.50185247 4425.23090975\n"," 4381.68016438 4938.61755111 4856.43280124 4427.39733825 4866.27123652\n"," 4582.3270979  4269.86433934 4809.85522796 5530.91290436 4580.52835571\n"," 4768.2516054  4908.79711378 4616.04970433 4756.40493754 4906.54138359\n"," 4777.30821385 4241.24785636 4795.19923893 5033.77852399 4943.29009996\n"," 4649.72215613 4725.25758183 4920.07710547 4683.40533529 4837.67662488\n"," 4764.71359973 4885.10988351 4973.7517862  4862.92329309 4715.14114196\n"," 4269.96249219 5906.96211182 4582.7862671  5884.65723322 4665.48623306\n"," 4807.76204992 4452.42646046 4835.64427314 4577.10195131 4673.41110531\n"," 4275.90331933 4842.90729858 4356.78248689 4457.91021261 4998.11734563\n"," 4543.00567483 5951.11961949 5132.51549164 4938.40973549 4532.06198262\n"," 4670.38677351 5686.46003278 4580.52835571 4861.17863785 4890.13425828\n"," 5001.93958946 4423.39057018 6053.10064936 4227.74354748 4721.99202038\n"," 4954.4684764  4366.72501312 4452.35634345 4776.22841396 4203.74665153\n"," 4705.25803987 4904.42572343 4640.81118982 4837.07809096 4744.94399656\n"," 5036.31529404 4713.01548949 4445.53270538 4842.87663302 4932.0298276\n"," 5026.74198576 4613.48281502 4730.97776028 4427.1436556  4576.8801817\n"," 5021.37478137 6017.19088797 4850.82319307 5816.89122297 4924.02687497\n"," 4127.90504379 4612.94339688 5732.86245489 4591.12244931 4989.50890478\n"," 4440.02177433 4987.57801022 4721.67132216 4763.50675157 4273.31518619\n"," 4766.28550159 4898.48513368 4741.26621028 4753.95406413 5839.48357408\n"," 4971.32835636 4834.24531373 4756.80507954 4281.60000927 4243.75133972\n"," 5132.51549164 4842.0780047  4555.33430807 4414.13320193 4676.56555489\n"," 4460.41198985 4498.29911975 5668.15754369 4619.24533046 4814.80791544\n"," 4922.84305166 4720.911483   4510.90499541 4967.1059359  4748.34527016\n"," 4698.81341684 4947.11520367 4592.23005577 4890.57073517 4825.43710001\n"," 4841.8781969  4845.95022141 4992.11223756 4760.5799756  5908.4798162\n"," 4849.64944218 4877.48697129 4297.91803482 5600.48738409 5931.56232173\n"," 4508.23463746 4270.6144643  5196.04395968 4958.8898293  4881.68324952\n"," 4746.6749041  4841.00598115 4692.01950911 4593.29099615 4452.79860241\n"," 4497.06597865 4241.40960623 4989.83286622 4643.10579448 4186.33321781\n"," 4680.69748569 4929.4001719  4455.10264096 5082.38961757 4681.25290671\n"," 4788.58359747 4448.28230261 4817.95211916 4932.21218131 4894.63459911\n"," 4309.272339   4838.45560647 4704.26388462 4952.65794868 4319.47728643\n"," 4481.00587599 5174.08123107 4956.74524467 5013.6452226  4327.50050631\n"," 4696.44108663 4639.36256692 4392.09590121 4805.63059979 4767.29335931\n"," 4928.72871457 4437.50199425 4561.13092651 4607.82038949 4596.41033071\n"," 4946.82912356 4529.97051101 4928.58719141 4622.92573049 4874.31503579\n"," 4449.38116387 4365.83001946 4458.23402577 4619.59204279 4854.77565645\n"," 4737.12918562 4804.55042929 4525.96620222 4973.83547654 4744.12127017\n"," 4392.89867281 4947.69934001 4949.0787283  4876.30019352 4857.22050891\n"," 4498.69413201 4846.07870055 4529.54199975 5878.97799679 4619.54834532\n"," 5713.76687516 4874.6671196  4771.09766194 4916.28875389 4923.99268424\n"," 6004.53274319 4742.57420955 4910.87391863 4947.0214596  4860.13984247\n"," 4433.79276149 4261.73460644 4359.81408449 5835.62098224]\n","In eval/whiteness_10\n"]},{"name":"stdout","output_type":"stream","text":["whiteness_10_result.npy\n","[0. 0. 0. ... 0. 0. 1.]\n","whiteness_10_is_ts_fmds.npy\n","[ True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n"," False  True  True  True  True  True False  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True False  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True False  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True False  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True False  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True]\n","whiteness_10_fmds.npy\n","[4681.29857664 4981.98611333 5593.94200378 4786.54243964 4766.07747135\n"," 4948.34563044 4597.76861577 4459.64412116 4514.50482999 5939.27719918\n"," 4977.48921606 4780.33073162 4776.81492479 4346.31646054 4918.56476007\n"," 4829.85315354 4799.34121776 5023.73784715 4956.29275129 4806.69828729\n"," 4776.17543808 5002.60069956 4473.06259291 5669.02468814 4491.11559608\n"," 4834.62527791 5777.18262327 4667.90580645 4377.46739131 4806.6235848\n"," 5956.94320571 4780.20210005 4834.4856459  5293.96815556 4660.17258052\n"," 6054.25672786 5006.98079377 4789.35814468 4474.41460535 4827.01665658\n"," 4831.82046068 5825.21428395 4714.62173947 4380.57527991 4619.863955\n"," 4675.8019096  4986.93012408 4791.16138292 4156.8729362  4852.53372912\n"," 4970.76138534 4458.04336783 4818.24995612 4374.86731081 4146.65739049\n"," 4311.50233059 4846.3323906  4831.95363479 4652.6704314  4784.41348787\n"," 4245.21342547 4953.86628211 4671.76210186 4310.58361001 4974.40675599\n"," 4989.04500098 5962.09130007 5184.24617587 4927.69396461 4826.89616721\n"," 4725.77475548 4609.05885936 4417.66554681 4722.84168904 4886.53556311\n"," 4828.61860164 4632.47763676 4977.00170891 4465.32618582 4859.0499033\n"," 4540.25813921 4771.79576205 4312.03134261 4742.65294683 5522.06533171\n"," 4550.50353438 4750.67446132 4799.90818257 4849.82534997 4677.9801708\n"," 4870.07968889 4723.92328647 4441.46689608 4792.75828782 4469.50362561\n"," 4633.83440482 4753.19541293 5022.47818099 4934.31899458 4943.13383576\n"," 5959.5575364  4652.07947559 4846.90216579 5362.99411007 4726.96840668\n"," 4766.08028419 5820.56574115 4896.22484469 4908.87830216 4984.91356918\n"," 4539.25644729 4927.47442457 4972.68721176 4853.67727771 4556.59062034\n"," 4549.72125346 4511.95028173 4344.97635347 4936.49530702 4753.69065534\n"," 4774.92825521 4592.24446418 4254.37591013 4888.44978224 4646.81651601\n"," 4831.09206601 4241.12740295 4874.00499679 4805.45475334 4622.34229302\n"," 4966.22395482 4776.98919501 4808.09312242 4928.94573517 4418.7660599\n"," 4479.22646032 5009.03107957 4951.72399619 4706.61034714 4822.63805826\n"," 6003.48623437 4593.85164641 6060.81373288 5038.71937764 5821.52327992\n"," 5672.3475974  5524.54363797 4647.53605645 4505.06662685 4919.747214\n"," 5738.20576251 4731.27751459 6031.55134199 4725.40722572 4423.38255856\n"," 4847.36886426 4553.23051564 4736.8055655  4658.24093091 4824.55906722\n"," 4956.10550252 4902.79648808 4832.51556775 4694.3126516  4991.25208898\n"," 4827.82765421 4861.75759754 5078.42553254 4729.05318954 4423.22390844\n"," 4974.62499784 4746.37311209 4743.40304444 4778.90711545 4731.89892087\n"," 4464.17377917 5586.60872875 4746.46446347 4765.73509279 4951.79125755\n"," 4721.85051345 4859.67119066 4809.57960983 4882.94812619 4928.15268873\n"," 4501.22640038 4438.22161967 4770.47659981 4935.12706682 4827.19496097\n"," 5507.11558643 4794.14862288 5846.57393289 4762.94427296 4947.9421727\n"," 4490.84950028 4418.65784951 5859.11503041 4697.24202429 4647.60593834\n"," 5909.52959136 4726.78475383 4619.7138252  4491.53671162 4734.53400872\n"," 4204.98318626 4696.59587524 4794.67086744 5106.86792397 4771.25204684\n"," 4435.1913959  4832.9970083  4926.23092801 4579.68150716 4785.66685866\n"," 5775.31900796 4448.72278546 4828.03971971 4292.35945728 4825.79109324\n"," 5531.10495576 4585.46934712 4780.81803832 4963.88781424 4257.05400447\n"," 5133.65432074 4429.27833429 4701.71533125 4571.36101203 4886.62908836\n"," 4846.58443046 4579.81804341 4898.76884232 4874.75360474 5186.56249018\n"," 4798.1191261  4249.6620177  5954.15838408 4302.49198917 4835.17836325\n"," 4912.44908802 4944.36055495 4486.11865431 4308.1144822  4796.51015534\n"," 4913.0554325  4478.26637078 4904.50126105 4548.67180067 4747.8772939\n"," 5001.46080595 4992.64491997 4699.23045899 4768.45867457 5974.30609108\n"," 4948.7931919  6017.60275503 5912.37045476 4590.57984215 4738.08847348\n"," 5017.51023751 5089.43959925 4785.92116501 5014.28418948 4411.55055131\n"," 4749.35413256 4906.68532573 5063.44305653 4835.36061752 4823.73813179\n"," 4997.53500827 4625.09804616 4952.12693365 4789.82614586 4975.0948832\n"," 4499.1159608  4880.23450895 4664.18839312 4785.00189329 4953.04062221\n"," 4961.23752177 5586.41037977 4498.82213708 4958.90145471 4928.25644302\n"," 5776.99714506 4320.78504051 4765.34119944 4766.60520703 4929.88561132\n"," 4599.08614682 4783.450307   5154.05891649 4941.86441365 4467.2339473\n"," 4497.52344386 4695.70258277 4814.98480309 5930.95605088 4554.65066182\n"," 4926.38944366 4751.85402009 5891.84931441 5857.59532066 4929.27505522\n"," 5020.74819098 4951.72399619 5042.25818989 4881.36505835 4528.7243231\n"," 4700.70623203 5051.50827776 4345.77861554 4836.2003502  4716.97393892\n"," 4706.25610619 4923.7942074  4623.81538241 4437.00979612 4661.56101082\n"," 4900.93815683 4485.22427374 4747.6417823  4762.40024871 4256.67728546\n"," 5656.48396316 4859.63979345 4850.6484205  4716.24320088 4447.34631119\n"," 4248.04809201 4524.79949721 4845.58917005 4560.32261601 4659.47904031\n"," 4658.77422379 4400.07348923 4380.90028602 5003.99908063 4739.50700047\n"," 4409.37821142 4474.22297752 4840.01523482 4835.82238085 4647.60350434\n"," 5836.0334637  4803.71939522 4481.84528933 4661.49306496 4858.40669212\n"," 4987.06975967 4956.03663111 4742.91006705 5229.88623559 4568.40008978\n"," 4813.50216312 4643.70919095 4826.37552822 4739.57241904 4963.29928177\n"," 4537.59361717 4828.52447827 4548.8231861  4710.51089086 4414.76948915\n"," 4512.028373   4738.45491927 4633.94075336 4604.81019433 5872.11666929\n"," 5030.24363262 4823.06922829 5056.32429323 4738.87929205 4478.97438193\n"," 5130.98297829 4853.16693851 5572.30002724 4719.78987166 4716.15729711\n"," 4887.58328199 5039.30795135 4811.70465763 5456.4573068  4853.20408651\n"," 4314.53474807 4258.02944484 4279.46109423 4689.06075814 4688.64774081\n"," 4735.39921831 4649.46751806 4597.63498124 4420.73889853 4960.34934443\n"," 5001.87120888 4871.53489861 4196.22141406 4633.65612184 4822.84659115\n"," 4702.03244372 4768.68383903 4463.13129654 4854.79062476 4457.29206347\n"," 4725.18306857 4934.6561017  4990.41249509 4633.56954643 4677.03870772\n"," 4604.39372673 4479.79703423 4468.3514943  4976.00785902 4530.39151497\n"," 4603.93641391 4666.89479426 4887.36000325 4653.83505932 4878.37181823\n"," 4691.38660697 4643.52717757 4876.92296064 4868.05638719 5851.12976282\n"," 4555.80233676 5962.72662289 5757.9461151  4359.62240505 4687.46901983\n"," 4485.55129938 4448.406314   4321.98188654 4560.13834561 4416.37527508\n"," 4482.91409683 4743.669352   4452.95271251 4602.99789242 4614.13619712\n"," 4312.77688761 4934.38957706 4683.18925612 4761.44405318 4944.28437947\n"," 4913.18921697 4828.41443516 4841.63580789 4614.11146536 5843.09884618\n"," 5954.13403125 4788.66860552 4905.21491038 5025.72843699 4381.69583267\n"," 5026.40494522 4909.77828191 4571.01993937 5001.53783428 5886.21416205\n"," 5035.82821618 4559.29346341 4565.65330394 4795.68791709 4807.84531703\n"," 4287.85715284 4574.03906879 4774.52513546 4728.56140134 5007.84022057\n"," 4974.40003481 5374.53962126 4669.40919183 4922.94943375 5884.78016302\n"," 5600.48738409 4673.42012964 4868.00227515 4849.66440695 4597.30373524\n"," 4791.43456758 4887.08775445 4956.67891692 4816.25163629 4715.16574488\n"," 4571.03550726 4893.7880104  4863.78818232 4700.56404972 4843.86364891\n"," 4630.79977562 5002.59659886 4227.30460473 4980.98968052 4571.74042401\n"," 4691.26766641 5044.47998805 4434.08759432 4220.3786929  5963.56727451\n"," 4619.76906946 4900.52370026 4445.15604242 4437.60872978 4882.44350324\n"," 4998.43848186 4709.08978321 5026.86612476 4451.58396935 4637.05476407\n"," 4718.47441011 4797.28774587 4545.90881361 4840.36881218 4900.00254817\n"," 4927.16541259 5375.76197005 5508.76514425 4743.08401865 4191.21625616\n"," 4324.15297067 4928.82822988 4619.18180494 4912.76987873 4406.39811353\n"," 4287.77922071 4698.5293233  5767.88636717 4906.42712545 4889.8715992\n"," 4807.68695104 4762.52402508 5205.89330907 5848.16601354 4323.06928533\n"," 4945.81739246 4379.60447213 4826.41100808 4882.00052573 4657.98213596\n"," 4872.71322426 4701.0086084  4698.11544016 4772.35567537 4920.84171863\n"," 4836.05080255 4733.36003383 5651.07974759 4763.29096048 5559.3149617\n"," 4710.53861447 4731.30502827 4475.34393504 5432.04096922 4827.77380732\n"," 5962.15554473 4266.27336369 4817.77896472 4694.01820482 4553.23088677\n"," 4853.77063877 4563.78536048 4951.5297426  4756.6541324  4711.49088781\n"," 4222.97299662 4834.76063634 5252.96419525 4783.09418867 5786.30367617\n"," 4383.64902283 4744.94937683 4941.38315751 4213.82501813 4622.9807148\n"," 4662.60991275 4450.38041898 4743.9366127  5099.00258003 4639.57244475\n"," 4800.29554718 4500.8826531  4441.08849771 4896.41257894 4515.69850001\n"," 4726.02313167 4870.55159689 4857.66556265 4703.87167584 4908.54189377\n"," 5354.6453825  4477.10166202 4474.20335948 4388.13676187 4624.92757755\n"," 5594.03577385 4482.888509   5077.56825102 4491.88593225 4515.39100268\n"," 4720.48385646 4579.9633144  4924.43033608 4849.47210255 4715.02291697\n"," 4552.37713298 4727.83477051 4935.96619293 6042.34891146 4373.00584934\n"," 4913.79966851 4749.33832762 5894.36040326 4779.58172288 4661.94744781\n"," 4954.80896975 5012.74723906 5784.72988258 4757.03921757 4965.95733462\n"," 4671.66179938 4792.92736874 4461.49386751 5000.63683015 4892.75747854\n"," 4611.03914102 4774.62105491 5677.01987222 4176.51355744 6084.91063979\n"," 4866.06790767 4788.48507673 4459.75572813 4890.98636714 5299.12600741\n"," 4585.18878756 4746.6749041  4968.30278483 4962.40498354 4249.22603677\n"," 4441.75740921 4656.13004632 4539.6140426  4864.8414879  4508.73763296\n"," 4927.93754723 4925.15485665 4892.46783125 4677.60390419 4886.42044297\n"," 4596.6328074  4736.10273285 4525.91793796 5023.27100852 4525.57337933\n"," 4707.40550327 4701.39276517 4527.49266233 4521.15409829 4743.95770962\n"," 4366.27115102 4903.60227765 4936.89737364 5527.17150027 5954.87449331\n"," 4462.79938957 4468.70595776 4657.50843494 4740.4272295  4795.15747171\n"," 4888.84638638 4601.91940228 4925.5018287  4902.49461879 4834.2918666\n"," 4329.64265347 4924.33715162 4792.63735795 4558.03712142 4840.71299352\n"," 5518.83145869 4545.94233233 4993.24418338 4288.18410121 5073.89148229\n"," 5050.0964341  4637.87694118 4855.60568554 4913.04768963 4456.49064982\n"," 4697.6026042  4570.78173717 4466.567019   4834.87832159 4700.23158333\n"," 4752.39668871 5933.02382197 5660.19047882 4529.99206177 4951.71355102\n"," 5099.25964596 4554.26265533 4773.58524523 4611.94266381 4643.36932428\n"," 4621.22062282 4507.07907805 4732.98955898 5459.27050106 5943.68549877\n"," 4510.66283174 4483.3559645  4528.82989227 4807.54950357 5804.66042863\n"," 4700.11355473 4981.43658115 4410.40030776 4773.57141364 4861.75759754\n"," 4698.1136093  4341.67168699 4789.89202066 4910.25507508 4871.03674225\n"," 4455.9720003  4997.08149112 4903.73753332 4635.663475   4384.10612126\n"," 4893.7880104  5573.18828209 4903.2336983  5007.26057866 4700.97978526\n"," 5047.43963476 4467.62527927 4921.4302126  5029.44173823 4285.60680454\n"," 4705.53453806 4795.14456149 5025.17057371 4402.04180244 4892.52110497\n"," 5019.09721535 4998.8074066  4298.6843804  5784.01293691 4949.98899696\n"," 4731.70828447 5093.32241945 5803.22613886 4963.38856732 4723.63733242\n"," 5874.08369408 5720.46053642 5008.78600594 5024.09565874 4510.53763173\n"," 4975.36449546 4870.94158864 4754.91041827 4912.74384906 4312.3109747\n"," 4207.64365056 4591.65948676 4843.86281502 4959.83675205 4912.7753005\n"," 4952.80913687 4738.93888913 4488.00661777 4745.29794184 4587.31563889\n"," 5825.21428395 5045.63136209 4709.35776767 4738.79805262 4910.14275922\n"," 4740.6308797  5737.55072026 4367.1743577  4510.53763173 4770.65064981\n"," 4817.64568817 4712.18982929 4703.49961239 4382.85673355 4774.07976207\n"," 4978.89728779 4941.46234392 4490.62804238 4912.84411347 4217.55172947\n"," 4825.38272385 4406.44570798 4437.45353427 4822.90267088 4613.73852645\n"," 4944.88278859 4712.41292078 4659.79347764 4276.56817987 4430.46206542\n"," 4913.15971446 5059.24465695 4339.99411334 4681.69510627 4978.50261326\n"," 4630.24702761 4806.75669626 4788.44879388 4922.55869263 4669.41440866\n"," 4517.77394627 4616.40183222 4913.3395912  4921.92203231 4795.17191919\n"," 4715.05401025 4955.35588611 4930.33215049 5986.28755497 5125.25580315\n"," 4706.90340483 4435.18967236 4369.73894553 4550.23669802 4937.58692525\n"," 4809.23088547 4763.24980473 4938.2993425  4756.49949313 4360.66254776\n"," 4950.69390336 4541.96597236 4925.5314485  4373.85214883 4983.1902118\n"," 4557.37249039 4885.40441051 4732.61419779 4791.72748449 4831.00603647\n"," 4856.54692097 4572.93253608 4685.73498029 4649.75893307 4939.70079921\n"," 4454.29136123 4780.4400466  4626.89032742 5992.70004956 4409.03619629\n"," 4965.95733462 4370.97885063 4414.84818975 5559.11016164 4782.29873649\n"," 4993.66069129 4913.70118002 4428.54426551 4641.34035078 4850.84588012\n"," 4853.72427614 4792.12041727 5006.46591221 4652.16152557 4446.3158612\n"," 4591.06185654 4899.6080076  4944.74690245 4689.08530359 4978.04106795\n"," 4900.18996385 4678.20213841 4926.9092079  4344.36097626 4809.93925208\n"," 4756.00931231 5477.82360389 4685.59825744 4619.25531844 4340.21012314\n"," 4778.25647765 4748.87703685 4680.12352075 4709.06629601 4684.87673261\n"," 4757.78698546 4787.34513948 4730.15660148 5989.93283518 4746.5795341\n"," 4843.7633796  4290.14168621 4926.7049511  4719.38836513 4830.88135389\n"," 4955.3091957  4829.01277597 4828.82424331 4839.8356209  5097.0171347\n"," 4439.20690551 4945.52710287 4969.54233045 5857.1157734  4950.2839169\n"," 4413.15416226 4307.98578929 4815.10413667 4735.55847516 6059.37870464\n"," 5032.12054464 5907.56972979 4922.06860252 4629.64456808 4512.01048807\n"," 4726.62411816 4513.0506555  4849.83147724 4733.73743853 5028.72496088\n"," 4419.29838743 4273.7540258  4867.95900782 4292.47528673 4231.42520698\n"," 4937.61328822 4884.66384372 4805.0436389  4739.3893146  4881.26165009\n"," 4770.65158127 4783.87300491 4610.37690277 4711.21203804 4951.93987786\n"," 4360.70281097 4997.44568497 4613.71748212 4593.60759485 4741.93974495\n"," 4437.70925894 4478.90938635 4943.47195668 4943.64094923 5224.31367199\n"," 4287.60034396 5583.42005492 4922.54232806 4815.15210051 4829.42518817\n"," 4763.27851414 4486.1785666  4895.58858128 4269.35673598 5022.0511015\n"," 4889.20781593 5013.97754898 4620.15288418 4942.3068694  4410.82498785\n"," 4777.95076473 4909.40195273 4512.98242656 4423.5628726  4435.08905708\n"," 4919.72077541 5998.63954587 4880.52836037 4994.5011994  5111.17090946\n"," 4401.78332347 4661.6962246  4175.59753016 4828.882251   4511.9667765\n"," 4791.29536397 4762.8538993  4555.67453052 4395.96893515 4637.42599695\n"," 4772.53328598 4774.92825521 4628.67791524 4806.70422717 4482.84761184\n"," 4366.94577306 4887.08775445 4906.41814648 6001.45564452 4654.39880389\n"," 4440.7954019  4918.55961641 4655.3123945  5690.94552298]\n"]}],"source":["fmd1.show_eval_infos()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-21T12:41:25.434153Z","start_time":"2022-07-21T12:41:25.051344Z"},"id":"yH85N7WZoVMj"},"outputs":[],"source":["a = np.load('/Volumes/My Passport_ssd_sg3/data_sets/fmnist/shirts/eval/rotation_90/rotation_90_result.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-21T12:41:28.413774Z","start_time":"2022-07-21T12:41:25.441222Z"},"id":"ljyF0NhyoVMl","outputId":"6ab2e6ed-55e2-49f0-8b03-bd70e6a985f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["47040001\n","23423503\n","23616498\n"]}],"source":["print(len(a))\n","print(len(np.nonzero(a)[0]))\n","print(len(a)-len(np.nonzero(a)[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3L01T73Czwp2"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-21T12:41:35.096597Z","start_time":"2022-07-21T12:41:28.424999Z"},"id":"J4R-TVmyoVMm","outputId":"917d0c49-78d0-46fc-e9f1-4e81d9cd69dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time elapsed:  0:00:03.245296\n","Time elapsed:  0:00:00.063163\n"]}],"source":["start = time.process_time()\n","\n","with open(f'/Volumes/My Passport_ssd_sg3/data_sets/fmnist/shirts/eval/brightness_10/brightness_10_0.pickle', 'rb') as f:\n","    EFMP_k = pickle.load(f)\n","\n","end = time.process_time()\n","print(\"Time elapsed: \", timedelta(seconds=end-start))\n","    \n","start = time.process_time()\n","\n","a = EFMP_k[0]\n","\n","end = time.process_time()\n","print(\"Time elapsed: \", timedelta(seconds=end-start))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"FMD_class.ipynb","provenance":[]},"gpuClass":"standard","hide_input":false,"kernelspec":{"display_name":"machine learning","language":"python","name":"ml"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}